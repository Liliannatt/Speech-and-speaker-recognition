{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "*run on full dataset, 1 epoch* \n",
    "\n",
    "* Dynamic, lmfcc, batch_size = 256, [256,256,256,256],lr = 0.0001, act_fun = \"relu\": 59.27%, 0.0338\n",
    "* Dynamic, lmfcc, batch_size = 256, [256,256,256,256],**lr = 0.0002**, act_fun = \"relu\": 62.59%, 0.0312\n",
    "* Dynamic, lmfcc, batch_size = 256, **[256, 512, 512, 256]**, lr = 0.0001, act_fun = \"relu\": 61.05%, 0.0325\n",
    "* Dynamic, lmfcc, **batch_size = 512**, [256, 512, 512, 256], **lr = 0.0002**, act_fun = \"relu\": 59.62%, 0.0336\n",
    "* Dynamic, lmfcc, batch_size = 256, **[256, 256, 256, 256, 256]**, lr = 0.0001, act_fun = \"relu\": 59.01%, 0.0338\n",
    "* Dynamic, **mspec**, batch_size = 256, [256,256,256,256],lr = 0.0001, act_fun = \"relu\": 55.39%, 0.00365\n",
    "* Dynamic, lmfcc, **batch_size = 512**, [256, 256, 256, 256], lr = 0.0001, act_fun = \"relu\": 55.27%, 0.0369\n",
    "* Dynamic, lmfcc, batch_size = 256, [256,256,256,256],lr = 0.0001, **act_fun = \"tanh\"**: 54.29%, 0.0377\n",
    "* Dynamic, lmfcc, batch_size = 256, **[128, 128, 128, 128]**, lr = 0.0001, act_fun = \"relu\": 53.61%, 0.0381\n",
    "* **Non-dynamic**, lmfcc, batch_size = 256, [256,256,256,256],lr = 0.0001, act_fun = \"relu\": 47.63%, 0.0421\n",
    "* **Non-dynamic**, **mspec**, batch_size = 256, [256,256,256,256],lr = 0.0001, act_fun = \"relu\": 43.50%, 0.0450\n",
    "\n",
    "\n",
    "**overall comparison**\n",
    "* dynamic > non dynamic\n",
    "* lmfcc > mspec\n",
    "* more nodes in each layer > less nodes\n",
    "* higher learning rate > lower learning rate\n",
    "* higher learning rate > bigger batch size && higher learning rate > bigger batchsize\n",
    "* more layers >= less layers\n",
    "* relu > tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_example import *\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "from loguru import logger\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    torch.cuda.set_device(device)\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup logging so that you can follow training using TensorBoard (see https://pytorch.org/docs/stable/tensorboard.html)\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model with dynamic mspec\n",
    "\n",
    "* dynamic mspec\n",
    "* active_func = relu\n",
    "* batch_size = 256\n",
    "* hidden_layer_list = [256, 256, 256, 256]\n",
    "* initial_lr = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-05-12 14:47:33.286\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m23\u001b[0m - \u001b[1mNet(\n",
      "  (activate): ReLU()\n",
      "  (layer1): Linear(in_features=280, out_features=256, bias=True)\n",
      "  (hidden_layers): Sequential(\n",
      "    (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (5): ReLU()\n",
      "  )\n",
      "  (header_layer): Linear(in_features=256, out_features=61, bias=True)\n",
      ")\u001b[0m\n",
      "\u001b[32m2024-05-12 14:47:33.287\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m24\u001b[0m - \u001b[1mnumber of prameters:\u001b[0m\n",
      "\u001b[32m2024-05-12 14:47:37.307\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 0000 / 5267, loss 0.00686, acc 0.39%, total time [00:00], one epoch time 00:06\u001b[0m\n",
      "\u001b[32m2024-05-12 14:47:38.450\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 0100 / 5267, loss 0.30431, acc 53.12%, total time [00:01], one epoch time 01:00\u001b[0m\n",
      "\u001b[32m2024-05-12 14:47:39.520\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 0200 / 5267, loss 0.06476, acc 55.86%, total time [00:02], one epoch time 00:56\u001b[0m\n",
      "\u001b[32m2024-05-12 14:47:40.465\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 0300 / 5267, loss 0.04994, acc 55.47%, total time [00:03], one epoch time 00:49\u001b[0m\n",
      "\u001b[32m2024-05-12 14:47:41.377\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 0400 / 5267, loss 0.04722, acc 53.91%, total time [00:04], one epoch time 00:47\u001b[0m\n",
      "\u001b[32m2024-05-12 14:47:42.314\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 0500 / 5267, loss 0.04613, acc 54.69%, total time [00:05], one epoch time 00:49\u001b[0m\n",
      "\u001b[32m2024-05-12 14:47:43.255\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 0600 / 5267, loss 0.04552, acc 57.81%, total time [00:06], one epoch time 00:49\u001b[0m\n",
      "\u001b[32m2024-05-12 14:47:44.273\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 0700 / 5267, loss 0.04483, acc 54.30%, total time [00:07], one epoch time 00:53\u001b[0m\n",
      "\u001b[32m2024-05-12 14:47:45.420\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 0800 / 5267, loss 0.04457, acc 56.25%, total time [00:08], one epoch time 01:00\u001b[0m\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 93\u001b[0m\n\u001b[0;32m     91\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m     92\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m---> 93\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;66;03m# accumulate the training loss\u001b[39;00m\n\u001b[0;32m     95\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\lilia\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:75\u001b[0m, in \u001b[0;36mLRScheduler.__init__.<locals>.with_counter.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     73\u001b[0m instance\u001b[38;5;241m.\u001b[39m_step_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     74\u001b[0m wrapped \u001b[38;5;241m=\u001b[39m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__get__\u001b[39m(instance, \u001b[38;5;28mcls\u001b[39m)\n\u001b[1;32m---> 75\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\lilia\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\optim\\optimizer.py:391\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    386\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    387\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    388\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    389\u001b[0m             )\n\u001b[1;32m--> 391\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    392\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    394\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\lilia\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\optim\\optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[1;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[1;32mc:\\Users\\lilia\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\optim\\adam.py:168\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    157\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m    159\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[0;32m    160\u001b[0m         group,\n\u001b[0;32m    161\u001b[0m         params_with_grad,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    165\u001b[0m         max_exp_avg_sqs,\n\u001b[0;32m    166\u001b[0m         state_steps)\n\u001b[1;32m--> 168\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    169\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    170\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    171\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    172\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    181\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    182\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    183\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    184\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    185\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    186\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    187\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    188\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    189\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32mc:\\Users\\lilia\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\optim\\adam.py:318\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    315\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    316\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 318\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    319\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    320\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    321\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    322\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    323\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    324\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    325\u001b[0m \u001b[43m     \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    326\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    327\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    328\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    329\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    330\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    331\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    332\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    333\u001b[0m \u001b[43m     \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    334\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    335\u001b[0m \u001b[43m     \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\lilia\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\optim\\adam.py:393\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[0;32m    390\u001b[0m     param \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mview_as_real(param)\n\u001b[0;32m    392\u001b[0m \u001b[38;5;66;03m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[1;32m--> 393\u001b[0m \u001b[43mexp_avg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlerp_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    394\u001b[0m exp_avg_sq\u001b[38;5;241m.\u001b[39mmul_(beta2)\u001b[38;5;241m.\u001b[39maddcmul_(grad, grad\u001b[38;5;241m.\u001b[39mconj(), value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta2)\n\u001b[0;32m    396\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m capturable \u001b[38;5;129;01mor\u001b[39;00m differentiable:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# set parameters and initiate net\n",
    "use_dynamic_features = True\n",
    "feature_type = 'mspec'\n",
    "activate_func = 'relu'\n",
    "batch_size = 256\n",
    "hidden_layer_list = [256, 256, 256, 256]\n",
    "initial_lr = 0.0001\n",
    "num_epochs = 1\n",
    "if feature_type == 'lmfcc':\n",
    "    feat_dim = 13\n",
    "elif feature_type == 'mspec':\n",
    "    feat_dim = 40\n",
    "\n",
    "if use_dynamic_features:\n",
    "    input_size = feat_dim * 7\n",
    "else:\n",
    "    input_size = feat_dim\n",
    "\n",
    "stateList = np.load('state_list.npy').tolist()\n",
    "num_cls = len(stateList)\n",
    "\n",
    "net = Net(input_size=input_size, num_cls=num_cls, hidden_layer_list=hidden_layer_list, activate_func=activate_func)\n",
    "logger.info(net)\n",
    "logger.info('number of prameters:', count_parameters(net))\n",
    "net = net.to(device)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=initial_lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=40, gamma=0.1)\n",
    "\n",
    "# load preprocessed data\n",
    "prepared_data_train = np.load('prepared_train_data.npz')\n",
    "if use_dynamic_features:\n",
    "    np_train_x = prepared_data_train[f'data_x_dynamic_{feature_type}']\n",
    "    np_train_y = prepared_data_train['data_y']\n",
    "else:\n",
    "    np_train_x = prepared_data_train[f'data_x_{feature_type}']\n",
    "    np_train_y = prepared_data_train['data_y']\n",
    "train_x = torch.tensor(np_train_x)\n",
    "train_y = F.one_hot(torch.tensor(np_train_y, dtype=torch.long), num_classes=num_cls).float()\n",
    "\n",
    "prepared_data_val = np.load('prepared_val_data.npz')\n",
    "if use_dynamic_features:\n",
    "    np_val_x = prepared_data_val[f'data_x_dynamic_{feature_type}']\n",
    "    np_val_y = prepared_data_val['data_y']\n",
    "else:\n",
    "    np_val_x = prepared_data_val[f'data_x_{feature_type}']\n",
    "    np_val_y = prepared_data_val['data_y']\n",
    "val_x = torch.tensor(np_val_x)\n",
    "val_y = F.one_hot(torch.tensor(np_val_y, dtype=torch.long), num_classes=num_cls).float()\n",
    "    \n",
    "prepared_data_test = np.load('prepared_test_data.npz')\n",
    "if use_dynamic_features:\n",
    "    np_test_x = prepared_data_test[f'data_x_dynamic_{feature_type}']\n",
    "    np_test_y = prepared_data_test['data_y']\n",
    "else:\n",
    "    np_test_x = prepared_data_test[f'data_x_{feature_type}']\n",
    "    np_test_y = prepared_data_test['data_y']\n",
    "test_x = torch.tensor(np_test_x)\n",
    "test_y = F.one_hot(torch.tensor(np_test_y, dtype=torch.long), num_classes=num_cls).float()\n",
    "\n",
    "# create the data loaders for training and validation sets\n",
    "train_dataset = torch.utils.data.TensorDataset(train_x, train_y)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataset = torch.utils.data.TensorDataset(val_x, val_y)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_dataset = torch.utils.data.TensorDataset(test_x, test_y)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# train the network\n",
    "st = time.time()\n",
    "last_st = time.time()\n",
    "log_interval = 100\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    net.train()\n",
    "    train_loss = 0.0\n",
    "    sub_train_loss = 0.0\n",
    "    epoch_acc = 0.0\n",
    "    cnt = 0\n",
    "    for idx, (inputs, labels) in enumerate(train_loader):\n",
    "\n",
    "        # move data from cpu to gpu (if available)\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # accumulate the training loss\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        sub_train_loss += loss.item()\n",
    "\n",
    "        if idx % log_interval == 0:\n",
    "            duration = time.time() - st\n",
    "            acc = calc_accuracy(labels, outputs)\n",
    "            epoch_acc += (acc * batch_size)\n",
    "            cnt += batch_size\n",
    "            logger.info(f'epoch {epoch:03d} / {num_epochs}, step {idx:04d} / {len(train_loader)}, loss {sub_train_loss / log_interval:.5f}, acc {acc*100:.2f}%, total time [{format_time(duration)}], one epoch time {format_time(len(train_loader) / log_interval * (time.time() - last_st))}')\n",
    "            sub_train_loss = 0.0\n",
    "            last_st = time.time()\n",
    "\n",
    "    # update the learning rate\n",
    "    scheduler.step()\n",
    "\n",
    "    # calculate the validation loss\n",
    "    net.eval()\n",
    "    val_loss = 0.0\n",
    "    val_epoch_acc = 0.0\n",
    "    val_cnt = 0\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0.0\n",
    "        logger.info('Validation...')\n",
    "        for inputs, labels in tqdm(val_loader):\n",
    "            # move data from cpu to gpu (if available)\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            acc = calc_accuracy(labels, outputs)\n",
    "            val_epoch_acc += (acc * batch_size)\n",
    "            val_cnt += len(inputs)\n",
    "\n",
    "    # print the epoch loss\n",
    "    train_loss /= len(train_loader)\n",
    "    val_loss /= len(val_loader)\n",
    "\n",
    "    train_acc = epoch_acc/cnt*100\n",
    "    val_acc = val_epoch_acc/val_cnt*100\n",
    "\n",
    "    logger.info(f'Epoch {epoch}: train_loss={train_loss:.4f}, train_acc={train_acc:.2f}%, val_loss={val_loss:.4f}, val_acc={val_acc:.2f}%, lr={scheduler.get_last_lr()[0]}')\n",
    "    writer.add_scalars('loss',{'train':train_loss,'val':val_loss},epoch)\n",
    "    \n",
    "# finally evaluate model on the test set here\n",
    "net.eval()\n",
    "test_loss = 0.0\n",
    "test_epoch_acc = 0.0\n",
    "test_cnt = 0\n",
    "logger.info('Testing...')\n",
    "with torch.no_grad():\n",
    "    test_loss = 0.0\n",
    "    for inputs, labels in tqdm(test_loader):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_loss += loss.item()\n",
    "        acc = calc_accuracy(labels, outputs)\n",
    "        test_epoch_acc += (acc * batch_size)\n",
    "        test_cnt += len(inputs)\n",
    "test_loss /= len(test_loader)\n",
    "test_acc = test_epoch_acc/test_cnt*100\n",
    "logger.info(f'Results: test_loss={test_loss:.4f}, test_acc={test_acc:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model with dynamic lmfcc\n",
    "\n",
    "* dynamic lmfcc\n",
    "* active_func = relu\n",
    "* batch_size = 256\n",
    "* hidden_layer_list = [256, 256, 256, 256]\n",
    "* initial_lr = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-05-10 10:12:56.914\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m23\u001b[0m - \u001b[1mNet(\n",
      "  (activate): ReLU()\n",
      "  (layer1): Linear(in_features=91, out_features=256, bias=True)\n",
      "  (hidden_layers): Sequential(\n",
      "    (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (5): ReLU()\n",
      "  )\n",
      "  (header_layer): Linear(in_features=256, out_features=61, bias=True)\n",
      ")\u001b[0m\n",
      "\u001b[32m2024-05-10 10:12:56.916\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m24\u001b[0m - \u001b[1mnumber of prameters:\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-05-10 10:12:58.491\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 0000 / 5267, loss 0.00694, acc 1.95%, total time [00:00], one epoch time 00:04\u001b[0m\n",
      "\u001b[32m2024-05-10 10:12:59.022\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 0100 / 5267, loss 0.39464, acc 13.67%, total time [00:00], one epoch time 00:27\u001b[0m\n",
      "\u001b[32m2024-05-10 10:12:59.555\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 0200 / 5267, loss 0.08115, acc 15.62%, total time [00:01], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:13:00.100\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 0300 / 5267, loss 0.07248, acc 21.09%, total time [00:01], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:13:00.645\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 0400 / 5267, loss 0.06695, acc 24.22%, total time [00:02], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:13:01.189\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 0500 / 5267, loss 0.06177, acc 29.30%, total time [00:02], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:13:01.734\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 0600 / 5267, loss 0.05709, acc 37.89%, total time [00:03], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:13:02.279\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 0700 / 5267, loss 0.05295, acc 37.50%, total time [00:03], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:13:02.805\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 0800 / 5267, loss 0.04993, acc 41.02%, total time [00:04], one epoch time 00:27\u001b[0m\n",
      "\u001b[32m2024-05-10 10:13:03.414\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 0900 / 5267, loss 0.04772, acc 43.36%, total time [00:05], one epoch time 00:32\u001b[0m\n",
      "\u001b[32m2024-05-10 10:13:03.947\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 1000 / 5267, loss 0.04679, acc 44.14%, total time [00:05], one epoch time 00:27\u001b[0m\n",
      "\u001b[32m2024-05-10 10:13:04.483\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 1100 / 5267, loss 0.04525, acc 42.97%, total time [00:06], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:13:05.016\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 1200 / 5267, loss 0.04450, acc 48.83%, total time [00:06], one epoch time 00:27\u001b[0m\n",
      "\u001b[32m2024-05-10 10:13:05.548\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 1300 / 5267, loss 0.04340, acc 46.09%, total time [00:07], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:13:06.094\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 1400 / 5267, loss 0.04341, acc 50.00%, total time [00:07], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:13:06.639\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 1500 / 5267, loss 0.04222, acc 51.17%, total time [00:08], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:13:07.184\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 1600 / 5267, loss 0.04142, acc 53.12%, total time [00:08], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:13:07.730\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 1700 / 5267, loss 0.04118, acc 47.27%, total time [00:09], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:13:08.277\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 1800 / 5267, loss 0.04043, acc 57.03%, total time [00:09], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:13:08.823\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 1900 / 5267, loss 0.04020, acc 51.56%, total time [00:10], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:13:09.435\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 2000 / 5267, loss 0.03950, acc 54.69%, total time [00:11], one epoch time 00:32\u001b[0m\n",
      "\u001b[32m2024-05-10 10:13:09.969\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 2100 / 5267, loss 0.03960, acc 48.44%, total time [00:11], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:13:10.514\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 2200 / 5267, loss 0.03866, acc 50.00%, total time [00:12], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:13:11.017\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 2300 / 5267, loss 0.03855, acc 57.81%, total time [00:12], one epoch time 00:26\u001b[0m\n",
      "\u001b[32m2024-05-10 10:13:11.540\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 2400 / 5267, loss 0.03810, acc 54.69%, total time [00:13], one epoch time 00:27\u001b[0m\n",
      "\u001b[32m2024-05-10 10:13:12.073\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 2500 / 5267, loss 0.03761, acc 54.69%, total time [00:13], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:13:12.609\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 2600 / 5267, loss 0.03757, acc 54.30%, total time [00:14], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:13:13.153\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 2700 / 5267, loss 0.03750, acc 58.59%, total time [00:14], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:13:13.698\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 2800 / 5267, loss 0.03704, acc 56.64%, total time [00:15], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:13:14.242\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 2900 / 5267, loss 0.03666, acc 55.47%, total time [00:15], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:13:14.786\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 3000 / 5267, loss 0.03652, acc 53.91%, total time [00:16], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:13:15.398\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 3100 / 5267, loss 0.03629, acc 58.20%, total time [00:16], one epoch time 00:32\u001b[0m\n",
      "\u001b[32m2024-05-10 10:13:15.943\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 3200 / 5267, loss 0.03602, acc 54.69%, total time [00:17], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:13:16.488\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 3300 / 5267, loss 0.03597, acc 50.39%, total time [00:18], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:13:17.033\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 3400 / 5267, loss 0.03561, acc 57.03%, total time [00:18], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:13:17.578\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 3500 / 5267, loss 0.03563, acc 54.30%, total time [00:19], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:13:18.123\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 3600 / 5267, loss 0.03514, acc 58.98%, total time [00:19], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:13:18.667\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 3700 / 5267, loss 0.03505, acc 54.30%, total time [00:20], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:13:19.212\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 3800 / 5267, loss 0.03507, acc 62.50%, total time [00:20], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:13:19.756\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 3900 / 5267, loss 0.03443, acc 54.69%, total time [00:21], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:13:20.301\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 4000 / 5267, loss 0.03470, acc 59.77%, total time [00:21], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:13:20.921\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 4100 / 5267, loss 0.03428, acc 61.72%, total time [00:22], one epoch time 00:32\u001b[0m\n",
      "\u001b[32m2024-05-10 10:13:21.452\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 4200 / 5267, loss 0.03444, acc 60.55%, total time [00:23], one epoch time 00:27\u001b[0m\n",
      "\u001b[32m2024-05-10 10:13:21.978\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 4300 / 5267, loss 0.03388, acc 57.81%, total time [00:23], one epoch time 00:27\u001b[0m\n",
      "\u001b[32m2024-05-10 10:13:22.511\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 4400 / 5267, loss 0.03387, acc 58.98%, total time [00:24], one epoch time 00:27\u001b[0m\n",
      "\u001b[32m2024-05-10 10:13:23.055\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 4500 / 5267, loss 0.03350, acc 60.16%, total time [00:24], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:13:23.601\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 4600 / 5267, loss 0.03365, acc 53.52%, total time [00:25], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:13:24.144\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 4700 / 5267, loss 0.03363, acc 61.33%, total time [00:25], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:13:24.688\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 4800 / 5267, loss 0.03337, acc 61.72%, total time [00:26], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:13:25.232\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 4900 / 5267, loss 0.03311, acc 57.81%, total time [00:26], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:13:25.776\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 5000 / 5267, loss 0.03301, acc 60.55%, total time [00:27], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:13:26.320\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 5100 / 5267, loss 0.03290, acc 62.11%, total time [00:27], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:13:26.933\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 5200 / 5267, loss 0.03280, acc 53.52%, total time [00:28], one epoch time 00:32\u001b[0m\n",
      "\u001b[32m2024-05-10 10:13:27.380\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m118\u001b[0m - \u001b[1mValidation...\u001b[0m\n",
      "100%|██████████| 622/622 [00:01<00:00, 408.42it/s]\n",
      "\u001b[32m2024-05-10 10:13:28.907\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m137\u001b[0m - \u001b[1mEpoch 0: train_loss=0.0478, train_acc=49.65%, val_loss=0.0334, val_acc=59.77%, lr=0.0001\u001b[0m\n",
      "\u001b[32m2024-05-10 10:13:28.908\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m145\u001b[0m - \u001b[1mTesting...\u001b[0m\n",
      "100%|██████████| 5965/5965 [00:15<00:00, 397.18it/s]\n",
      "\u001b[32m2024-05-10 10:13:43.931\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mResults: test_loss=0.0338, test_acc=59.27%\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# set parameters and initiate net\n",
    "use_dynamic_features = True\n",
    "feature_type = 'lmfcc'\n",
    "activate_func = 'relu'\n",
    "batch_size = 256\n",
    "hidden_layer_list = [256, 256, 256, 256]\n",
    "initial_lr = 0.0001\n",
    "num_epochs = 1\n",
    "if feature_type == 'lmfcc':\n",
    "    feat_dim = 13\n",
    "elif feature_type == 'mspec':\n",
    "    feat_dim = 40\n",
    "\n",
    "if use_dynamic_features:\n",
    "    input_size = feat_dim * 7\n",
    "else:\n",
    "    input_size = feat_dim\n",
    "\n",
    "stateList = np.load('state_list.npy').tolist()\n",
    "num_cls = len(stateList)\n",
    "\n",
    "net = Net(input_size=input_size, num_cls=num_cls, hidden_layer_list=hidden_layer_list, activate_func=activate_func)\n",
    "logger.info(net)\n",
    "logger.info('number of prameters:', count_parameters(net))\n",
    "net = net.to(device)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=initial_lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=40, gamma=0.1)\n",
    "\n",
    "# load preprocessed data\n",
    "prepared_data_train = np.load('prepared_train_data.npz')\n",
    "if use_dynamic_features:\n",
    "    np_train_x = prepared_data_train[f'data_x_dynamic_{feature_type}']\n",
    "    np_train_y = prepared_data_train['data_y']\n",
    "else:\n",
    "    np_train_x = prepared_data_train[f'data_x_{feature_type}']\n",
    "    np_train_y = prepared_data_train['data_y']\n",
    "train_x = torch.tensor(np_train_x)\n",
    "train_y = F.one_hot(torch.tensor(np_train_y, dtype=torch.long), num_classes=num_cls).float()\n",
    "\n",
    "prepared_data_val = np.load('prepared_val_data.npz')\n",
    "if use_dynamic_features:\n",
    "    np_val_x = prepared_data_val[f'data_x_dynamic_{feature_type}']\n",
    "    np_val_y = prepared_data_val['data_y']\n",
    "else:\n",
    "    np_val_x = prepared_data_val[f'data_x_{feature_type}']\n",
    "    np_val_y = prepared_data_val['data_y']\n",
    "val_x = torch.tensor(np_val_x)\n",
    "val_y = F.one_hot(torch.tensor(np_val_y, dtype=torch.long), num_classes=num_cls).float()\n",
    "    \n",
    "prepared_data_test = np.load('prepared_test_data.npz')\n",
    "if use_dynamic_features:\n",
    "    np_test_x = prepared_data_test[f'data_x_dynamic_{feature_type}']\n",
    "    np_test_y = prepared_data_test['data_y']\n",
    "else:\n",
    "    np_test_x = prepared_data_test[f'data_x_{feature_type}']\n",
    "    np_test_y = prepared_data_test['data_y']\n",
    "test_x = torch.tensor(np_test_x)\n",
    "test_y = F.one_hot(torch.tensor(np_test_y, dtype=torch.long), num_classes=num_cls).float()\n",
    "\n",
    "# create the data loaders for training and validation sets\n",
    "train_dataset = torch.utils.data.TensorDataset(train_x, train_y)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataset = torch.utils.data.TensorDataset(val_x, val_y)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_dataset = torch.utils.data.TensorDataset(test_x, test_y)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# train the network\n",
    "st = time.time()\n",
    "last_st = time.time()\n",
    "log_interval = 100\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    net.train()\n",
    "    train_loss = 0.0\n",
    "    sub_train_loss = 0.0\n",
    "    epoch_acc = 0.0\n",
    "    cnt = 0\n",
    "    for idx, (inputs, labels) in enumerate(train_loader):\n",
    "\n",
    "        # move data from cpu to gpu (if available)\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # accumulate the training loss\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        sub_train_loss += loss.item()\n",
    "\n",
    "        if idx % log_interval == 0:\n",
    "            duration = time.time() - st\n",
    "            acc = calc_accuracy(labels, outputs)\n",
    "            epoch_acc += (acc * batch_size)\n",
    "            cnt += batch_size\n",
    "            logger.info(f'epoch {epoch:03d} / {num_epochs}, step {idx:04d} / {len(train_loader)}, loss {sub_train_loss / log_interval:.5f}, acc {acc*100:.2f}%, total time [{format_time(duration)}], one epoch time {format_time(len(train_loader) / log_interval * (time.time() - last_st))}')\n",
    "            sub_train_loss = 0.0\n",
    "            last_st = time.time()\n",
    "\n",
    "    # update the learning rate\n",
    "    scheduler.step()\n",
    "\n",
    "    # calculate the validation loss\n",
    "    net.eval()\n",
    "    val_loss = 0.0\n",
    "    val_epoch_acc = 0.0\n",
    "    val_cnt = 0\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0.0\n",
    "        logger.info('Validation...')\n",
    "        for inputs, labels in tqdm(val_loader):\n",
    "            # move data from cpu to gpu (if available)\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            acc = calc_accuracy(labels, outputs)\n",
    "            val_epoch_acc += (acc * batch_size)\n",
    "            val_cnt += len(inputs)\n",
    "\n",
    "    # print the epoch loss\n",
    "    train_loss /= len(train_loader)\n",
    "    val_loss /= len(val_loader)\n",
    "\n",
    "    train_acc = epoch_acc/cnt*100\n",
    "    val_acc = val_epoch_acc/val_cnt*100\n",
    "\n",
    "    logger.info(f'Epoch {epoch}: train_loss={train_loss:.4f}, train_acc={train_acc:.2f}%, val_loss={val_loss:.4f}, val_acc={val_acc:.2f}%, lr={scheduler.get_last_lr()[0]}')\n",
    "    writer.add_scalars('loss',{'train':train_loss,'val':val_loss},epoch)\n",
    "    \n",
    "# finally evaluate model on the test set here\n",
    "net.eval()\n",
    "test_loss = 0.0\n",
    "test_epoch_acc = 0.0\n",
    "test_cnt = 0\n",
    "logger.info('Testing...')\n",
    "with torch.no_grad():\n",
    "    test_loss = 0.0\n",
    "    for inputs, labels in tqdm(test_loader):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_loss += loss.item()\n",
    "        acc = calc_accuracy(labels, outputs)\n",
    "        test_epoch_acc += (acc * batch_size)\n",
    "        test_cnt += len(inputs)\n",
    "test_loss /= len(test_loader)\n",
    "test_acc = test_epoch_acc/test_cnt*100\n",
    "logger.info(f'Results: test_loss={test_loss:.4f}, test_acc={test_acc:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## non dynamic model with mspec\n",
    "\n",
    "* non dynamic mspec\n",
    "* active_func = relu\n",
    "* batch_size = 256\n",
    "* hidden_layer_list = [256, 256, 256, 256]\n",
    "* initial_lr = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-05-10 10:14:03.987\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m23\u001b[0m - \u001b[1mNet(\n",
      "  (activate): ReLU()\n",
      "  (layer1): Linear(in_features=40, out_features=256, bias=True)\n",
      "  (hidden_layers): Sequential(\n",
      "    (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (5): ReLU()\n",
      "  )\n",
      "  (header_layer): Linear(in_features=256, out_features=61, bias=True)\n",
      ")\u001b[0m\n",
      "\u001b[32m2024-05-10 10:14:03.988\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m24\u001b[0m - \u001b[1mnumber of prameters:\u001b[0m\n",
      "\u001b[32m2024-05-10 10:14:04.837\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 0000 / 5267, loss 0.00693, acc 0.39%, total time [00:00], one epoch time 00:04\u001b[0m\n",
      "\u001b[32m2024-05-10 10:14:05.400\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 0100 / 5267, loss 0.38057, acc 12.89%, total time [00:00], one epoch time 00:29\u001b[0m\n",
      "\u001b[32m2024-05-10 10:14:05.955\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 0200 / 5267, loss 0.09344, acc 13.28%, total time [00:01], one epoch time 00:29\u001b[0m\n",
      "\u001b[32m2024-05-10 10:14:06.510\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 0300 / 5267, loss 0.07348, acc 20.31%, total time [00:01], one epoch time 00:29\u001b[0m\n",
      "\u001b[32m2024-05-10 10:14:07.064\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 0400 / 5267, loss 0.06614, acc 16.80%, total time [00:02], one epoch time 00:29\u001b[0m\n",
      "\u001b[32m2024-05-10 10:14:07.612\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 0500 / 5267, loss 0.06261, acc 26.95%, total time [00:02], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:14:08.151\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 0600 / 5267, loss 0.06036, acc 21.88%, total time [00:03], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:14:08.690\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 0700 / 5267, loss 0.05886, acc 23.83%, total time [00:03], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:14:09.309\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 0800 / 5267, loss 0.05799, acc 32.81%, total time [00:04], one epoch time 00:32\u001b[0m\n",
      "\u001b[32m2024-05-10 10:14:09.863\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 0900 / 5267, loss 0.05680, acc 33.59%, total time [00:05], one epoch time 00:29\u001b[0m\n",
      "\u001b[32m2024-05-10 10:14:10.416\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 1000 / 5267, loss 0.05525, acc 29.30%, total time [00:05], one epoch time 00:29\u001b[0m\n",
      "\u001b[32m2024-05-10 10:14:10.970\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 1100 / 5267, loss 0.05444, acc 38.28%, total time [00:06], one epoch time 00:29\u001b[0m\n",
      "\u001b[32m2024-05-10 10:14:11.525\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 1200 / 5267, loss 0.05370, acc 28.52%, total time [00:06], one epoch time 00:29\u001b[0m\n",
      "\u001b[32m2024-05-10 10:14:12.079\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 1300 / 5267, loss 0.05303, acc 32.42%, total time [00:07], one epoch time 00:29\u001b[0m\n",
      "\u001b[32m2024-05-10 10:14:12.631\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 1400 / 5267, loss 0.05165, acc 29.30%, total time [00:07], one epoch time 00:29\u001b[0m\n",
      "\u001b[32m2024-05-10 10:14:13.173\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 1500 / 5267, loss 0.05103, acc 31.64%, total time [00:08], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:14:13.715\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 1600 / 5267, loss 0.05065, acc 31.25%, total time [00:08], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:14:14.255\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 1700 / 5267, loss 0.05023, acc 29.30%, total time [00:09], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:14:14.876\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 1800 / 5267, loss 0.04973, acc 35.55%, total time [00:10], one epoch time 00:32\u001b[0m\n",
      "\u001b[32m2024-05-10 10:14:15.403\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 1900 / 5267, loss 0.04949, acc 34.77%, total time [00:10], one epoch time 00:27\u001b[0m\n",
      "\u001b[32m2024-05-10 10:14:15.944\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 2000 / 5267, loss 0.04922, acc 41.80%, total time [00:11], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:14:16.485\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 2100 / 5267, loss 0.04921, acc 36.72%, total time [00:11], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:14:17.024\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 2200 / 5267, loss 0.04871, acc 35.16%, total time [00:12], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:14:17.564\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 2300 / 5267, loss 0.04863, acc 39.45%, total time [00:12], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:14:18.105\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 2400 / 5267, loss 0.04818, acc 41.02%, total time [00:13], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:14:18.644\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 2500 / 5267, loss 0.04821, acc 39.45%, total time [00:13], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:14:19.183\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 2600 / 5267, loss 0.04802, acc 35.94%, total time [00:14], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:14:19.722\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 2700 / 5267, loss 0.04797, acc 35.55%, total time [00:14], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:14:20.263\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 2800 / 5267, loss 0.04760, acc 43.36%, total time [00:15], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:14:20.876\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 2900 / 5267, loss 0.04785, acc 37.50%, total time [00:16], one epoch time 00:32\u001b[0m\n",
      "\u001b[32m2024-05-10 10:14:21.393\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 3000 / 5267, loss 0.04737, acc 41.41%, total time [00:16], one epoch time 00:27\u001b[0m\n",
      "\u001b[32m2024-05-10 10:14:21.921\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 3100 / 5267, loss 0.04734, acc 35.94%, total time [00:17], one epoch time 00:27\u001b[0m\n",
      "\u001b[32m2024-05-10 10:14:22.463\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 3200 / 5267, loss 0.04735, acc 45.70%, total time [00:17], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:14:23.005\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 3300 / 5267, loss 0.04670, acc 41.80%, total time [00:18], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:14:23.546\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 3400 / 5267, loss 0.04656, acc 44.53%, total time [00:18], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:14:24.087\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 3500 / 5267, loss 0.04664, acc 42.97%, total time [00:19], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:14:24.629\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 3600 / 5267, loss 0.04620, acc 43.36%, total time [00:19], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:14:25.171\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 3700 / 5267, loss 0.04596, acc 37.11%, total time [00:20], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:14:25.713\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 3800 / 5267, loss 0.04609, acc 45.70%, total time [00:20], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:14:26.256\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 3900 / 5267, loss 0.04582, acc 43.36%, total time [00:21], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:14:26.863\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 4000 / 5267, loss 0.04538, acc 38.67%, total time [00:22], one epoch time 00:31\u001b[0m\n",
      "\u001b[32m2024-05-10 10:14:27.392\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 4100 / 5267, loss 0.04571, acc 39.06%, total time [00:22], one epoch time 00:27\u001b[0m\n",
      "\u001b[32m2024-05-10 10:14:27.934\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 4200 / 5267, loss 0.04543, acc 37.50%, total time [00:23], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:14:28.476\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 4300 / 5267, loss 0.04551, acc 44.92%, total time [00:23], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:14:29.018\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 4400 / 5267, loss 0.04522, acc 48.83%, total time [00:24], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:14:29.560\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 4500 / 5267, loss 0.04534, acc 41.41%, total time [00:24], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:14:30.101\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 4600 / 5267, loss 0.04515, acc 41.02%, total time [00:25], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:14:30.643\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 4700 / 5267, loss 0.04491, acc 44.53%, total time [00:25], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:14:31.185\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 4800 / 5267, loss 0.04507, acc 43.36%, total time [00:26], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:14:31.725\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 4900 / 5267, loss 0.04529, acc 44.92%, total time [00:26], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:14:32.267\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 5000 / 5267, loss 0.04465, acc 48.05%, total time [00:27], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:14:32.874\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 5100 / 5267, loss 0.04485, acc 45.70%, total time [00:28], one epoch time 00:31\u001b[0m\n",
      "\u001b[32m2024-05-10 10:14:33.405\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 5200 / 5267, loss 0.04409, acc 43.36%, total time [00:28], one epoch time 00:27\u001b[0m\n",
      "\u001b[32m2024-05-10 10:14:33.858\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m118\u001b[0m - \u001b[1mValidation...\u001b[0m\n",
      "100%|██████████| 622/622 [00:01<00:00, 411.83it/s]\n",
      "\u001b[32m2024-05-10 10:14:35.373\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m137\u001b[0m - \u001b[1mEpoch 0: train_loss=0.0570, train_acc=35.70%, val_loss=0.0452, val_acc=43.18%, lr=0.0001\u001b[0m\n",
      "\u001b[32m2024-05-10 10:14:35.374\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m145\u001b[0m - \u001b[1mTesting...\u001b[0m\n",
      "100%|██████████| 5965/5965 [00:14<00:00, 401.09it/s]\n",
      "\u001b[32m2024-05-10 10:14:50.251\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mResults: test_loss=0.0450, test_acc=43.50%\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# set parameters and initiate net\n",
    "use_dynamic_features = False\n",
    "feature_type = 'mspec'\n",
    "activate_func = 'relu'\n",
    "batch_size = 256\n",
    "hidden_layer_list = [256, 256, 256, 256]\n",
    "initial_lr = 0.0001\n",
    "num_epochs = 1\n",
    "if feature_type == 'lmfcc':\n",
    "    feat_dim = 13\n",
    "elif feature_type == 'mspec':\n",
    "    feat_dim = 40\n",
    "\n",
    "if use_dynamic_features:\n",
    "    input_size = feat_dim * 7\n",
    "else:\n",
    "    input_size = feat_dim\n",
    "\n",
    "stateList = np.load('state_list.npy').tolist()\n",
    "num_cls = len(stateList)\n",
    "\n",
    "net = Net(input_size=input_size, num_cls=num_cls, hidden_layer_list=hidden_layer_list, activate_func=activate_func)\n",
    "logger.info(net)\n",
    "logger.info('number of prameters:', count_parameters(net))\n",
    "net = net.to(device)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=initial_lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=40, gamma=0.1)\n",
    "\n",
    "# load preprocessed data\n",
    "prepared_data_train = np.load('prepared_train_data.npz')\n",
    "if use_dynamic_features:\n",
    "    np_train_x = prepared_data_train[f'data_x_dynamic_{feature_type}']\n",
    "    np_train_y = prepared_data_train['data_y']\n",
    "else:\n",
    "    np_train_x = prepared_data_train[f'data_x_{feature_type}']\n",
    "    np_train_y = prepared_data_train['data_y']\n",
    "train_x = torch.tensor(np_train_x)\n",
    "train_y = F.one_hot(torch.tensor(np_train_y, dtype=torch.long), num_classes=num_cls).float()\n",
    "\n",
    "prepared_data_val = np.load('prepared_val_data.npz')\n",
    "if use_dynamic_features:\n",
    "    np_val_x = prepared_data_val[f'data_x_dynamic_{feature_type}']\n",
    "    np_val_y = prepared_data_val['data_y']\n",
    "else:\n",
    "    np_val_x = prepared_data_val[f'data_x_{feature_type}']\n",
    "    np_val_y = prepared_data_val['data_y']\n",
    "val_x = torch.tensor(np_val_x)\n",
    "val_y = F.one_hot(torch.tensor(np_val_y, dtype=torch.long), num_classes=num_cls).float()\n",
    "    \n",
    "prepared_data_test = np.load('prepared_test_data.npz')\n",
    "if use_dynamic_features:\n",
    "    np_test_x = prepared_data_test[f'data_x_dynamic_{feature_type}']\n",
    "    np_test_y = prepared_data_test['data_y']\n",
    "else:\n",
    "    np_test_x = prepared_data_test[f'data_x_{feature_type}']\n",
    "    np_test_y = prepared_data_test['data_y']\n",
    "test_x = torch.tensor(np_test_x)\n",
    "test_y = F.one_hot(torch.tensor(np_test_y, dtype=torch.long), num_classes=num_cls).float()\n",
    "\n",
    "# create the data loaders for training and validation sets\n",
    "train_dataset = torch.utils.data.TensorDataset(train_x, train_y)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataset = torch.utils.data.TensorDataset(val_x, val_y)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_dataset = torch.utils.data.TensorDataset(test_x, test_y)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# train the network\n",
    "st = time.time()\n",
    "last_st = time.time()\n",
    "log_interval = 100\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    net.train()\n",
    "    train_loss = 0.0\n",
    "    sub_train_loss = 0.0\n",
    "    epoch_acc = 0.0\n",
    "    cnt = 0\n",
    "    for idx, (inputs, labels) in enumerate(train_loader):\n",
    "\n",
    "        # move data from cpu to gpu (if available)\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # accumulate the training loss\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        sub_train_loss += loss.item()\n",
    "\n",
    "        if idx % log_interval == 0:\n",
    "            duration = time.time() - st\n",
    "            acc = calc_accuracy(labels, outputs)\n",
    "            epoch_acc += (acc * batch_size)\n",
    "            cnt += batch_size\n",
    "            logger.info(f'epoch {epoch:03d} / {num_epochs}, step {idx:04d} / {len(train_loader)}, loss {sub_train_loss / log_interval:.5f}, acc {acc*100:.2f}%, total time [{format_time(duration)}], one epoch time {format_time(len(train_loader) / log_interval * (time.time() - last_st))}')\n",
    "            sub_train_loss = 0.0\n",
    "            last_st = time.time()\n",
    "\n",
    "    # update the learning rate\n",
    "    scheduler.step()\n",
    "\n",
    "    # calculate the validation loss\n",
    "    net.eval()\n",
    "    val_loss = 0.0\n",
    "    val_epoch_acc = 0.0\n",
    "    val_cnt = 0\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0.0\n",
    "        logger.info('Validation...')\n",
    "        for inputs, labels in tqdm(val_loader):\n",
    "            # move data from cpu to gpu (if available)\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            acc = calc_accuracy(labels, outputs)\n",
    "            val_epoch_acc += (acc * batch_size)\n",
    "            val_cnt += len(inputs)\n",
    "\n",
    "    # print the epoch loss\n",
    "    train_loss /= len(train_loader)\n",
    "    val_loss /= len(val_loader)\n",
    "\n",
    "    train_acc = epoch_acc/cnt*100\n",
    "    val_acc = val_epoch_acc/val_cnt*100\n",
    "\n",
    "    logger.info(f'Epoch {epoch}: train_loss={train_loss:.4f}, train_acc={train_acc:.2f}%, val_loss={val_loss:.4f}, val_acc={val_acc:.2f}%, lr={scheduler.get_last_lr()[0]}')\n",
    "    writer.add_scalars('loss',{'train':train_loss,'val':val_loss},epoch)\n",
    "    \n",
    "# finally evaluate model on the test set here\n",
    "net.eval()\n",
    "test_loss = 0.0\n",
    "test_epoch_acc = 0.0\n",
    "test_cnt = 0\n",
    "logger.info('Testing...')\n",
    "with torch.no_grad():\n",
    "    test_loss = 0.0\n",
    "    for inputs, labels in tqdm(test_loader):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_loss += loss.item()\n",
    "        acc = calc_accuracy(labels, outputs)\n",
    "        test_epoch_acc += (acc * batch_size)\n",
    "        test_cnt += len(inputs)\n",
    "test_loss /= len(test_loader)\n",
    "test_acc = test_epoch_acc/test_cnt*100\n",
    "logger.info(f'Results: test_loss={test_loss:.4f}, test_acc={test_acc:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model with non dynamic lmfcc\n",
    "\n",
    "* non dynamic lmfcc\n",
    "* active_func = relu\n",
    "* batch_size = 256\n",
    "* hidden_layer_list = [256, 256, 256, 256]\n",
    "* initial_lr = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-05-10 10:15:19.674\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m23\u001b[0m - \u001b[1mNet(\n",
      "  (activate): ReLU()\n",
      "  (layer1): Linear(in_features=13, out_features=256, bias=True)\n",
      "  (hidden_layers): Sequential(\n",
      "    (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (5): ReLU()\n",
      "  )\n",
      "  (header_layer): Linear(in_features=256, out_features=61, bias=True)\n",
      ")\u001b[0m\n",
      "\u001b[32m2024-05-10 10:15:19.676\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m24\u001b[0m - \u001b[1mnumber of prameters:\u001b[0m\n",
      "\u001b[32m2024-05-10 10:15:20.303\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 0000 / 5267, loss 0.00695, acc 1.56%, total time [00:00], one epoch time 00:04\u001b[0m\n",
      "\u001b[32m2024-05-10 10:15:20.896\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 0100 / 5267, loss 0.40154, acc 10.55%, total time [00:00], one epoch time 00:31\u001b[0m\n",
      "\u001b[32m2024-05-10 10:15:21.465\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 0200 / 5267, loss 0.07818, acc 12.89%, total time [00:01], one epoch time 00:29\u001b[0m\n",
      "\u001b[32m2024-05-10 10:15:22.034\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 0300 / 5267, loss 0.07298, acc 13.67%, total time [00:01], one epoch time 00:29\u001b[0m\n",
      "\u001b[32m2024-05-10 10:15:22.603\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 0400 / 5267, loss 0.06883, acc 19.53%, total time [00:02], one epoch time 00:29\u001b[0m\n",
      "\u001b[32m2024-05-10 10:15:23.172\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 0500 / 5267, loss 0.06496, acc 22.27%, total time [00:02], one epoch time 00:29\u001b[0m\n",
      "\u001b[32m2024-05-10 10:15:23.816\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 0600 / 5267, loss 0.06164, acc 32.81%, total time [00:03], one epoch time 00:33\u001b[0m\n",
      "\u001b[32m2024-05-10 10:15:24.346\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 0700 / 5267, loss 0.05726, acc 33.20%, total time [00:04], one epoch time 00:27\u001b[0m\n",
      "\u001b[32m2024-05-10 10:15:24.886\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 0800 / 5267, loss 0.05449, acc 34.77%, total time [00:04], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:15:25.429\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 0900 / 5267, loss 0.05187, acc 41.41%, total time [00:05], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:15:25.970\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 1000 / 5267, loss 0.05038, acc 37.50%, total time [00:05], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:15:26.514\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 1100 / 5267, loss 0.04922, acc 38.67%, total time [00:06], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:15:27.056\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 1200 / 5267, loss 0.04817, acc 47.27%, total time [00:06], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:15:27.598\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 1300 / 5267, loss 0.04737, acc 41.80%, total time [00:07], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:15:28.141\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 1400 / 5267, loss 0.04699, acc 44.53%, total time [00:07], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:15:28.684\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 1500 / 5267, loss 0.04629, acc 45.31%, total time [00:08], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:15:29.226\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 1600 / 5267, loss 0.04614, acc 45.70%, total time [00:09], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:15:29.840\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 1700 / 5267, loss 0.04550, acc 43.75%, total time [00:09], one epoch time 00:32\u001b[0m\n",
      "\u001b[32m2024-05-10 10:15:30.368\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 1800 / 5267, loss 0.04536, acc 46.09%, total time [00:10], one epoch time 00:27\u001b[0m\n",
      "\u001b[32m2024-05-10 10:15:30.910\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 1900 / 5267, loss 0.04549, acc 47.66%, total time [00:10], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:15:31.450\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 2000 / 5267, loss 0.04507, acc 47.27%, total time [00:11], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:15:31.992\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 2100 / 5267, loss 0.04434, acc 44.53%, total time [00:11], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:15:32.534\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 2200 / 5267, loss 0.04441, acc 45.31%, total time [00:12], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:15:33.075\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 2300 / 5267, loss 0.04442, acc 47.27%, total time [00:12], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:15:33.616\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 2400 / 5267, loss 0.04429, acc 46.88%, total time [00:13], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:15:34.157\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 2500 / 5267, loss 0.04382, acc 43.36%, total time [00:13], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:15:34.698\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 2600 / 5267, loss 0.04405, acc 47.66%, total time [00:14], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:15:35.252\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 2700 / 5267, loss 0.04381, acc 45.31%, total time [00:15], one epoch time 00:29\u001b[0m\n",
      "\u001b[32m2024-05-10 10:15:35.867\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 2800 / 5267, loss 0.04341, acc 43.75%, total time [00:15], one epoch time 00:32\u001b[0m\n",
      "\u001b[32m2024-05-10 10:15:36.408\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 2900 / 5267, loss 0.04345, acc 47.27%, total time [00:16], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:15:36.949\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 3000 / 5267, loss 0.04304, acc 44.14%, total time [00:16], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:15:37.490\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 3100 / 5267, loss 0.04344, acc 49.61%, total time [00:17], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:15:38.032\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 3200 / 5267, loss 0.04338, acc 44.92%, total time [00:17], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:15:38.575\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 3300 / 5267, loss 0.04304, acc 46.88%, total time [00:18], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:15:39.117\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 3400 / 5267, loss 0.04278, acc 51.17%, total time [00:18], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:15:39.659\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 3500 / 5267, loss 0.04254, acc 45.70%, total time [00:19], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:15:40.200\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 3600 / 5267, loss 0.04258, acc 48.05%, total time [00:19], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:15:40.743\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 3700 / 5267, loss 0.04284, acc 48.05%, total time [00:20], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:15:41.284\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 3800 / 5267, loss 0.04289, acc 48.05%, total time [00:21], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:15:41.894\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 3900 / 5267, loss 0.04211, acc 42.19%, total time [00:21], one epoch time 00:32\u001b[0m\n",
      "\u001b[32m2024-05-10 10:15:42.433\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 4000 / 5267, loss 0.04262, acc 46.88%, total time [00:22], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:15:42.972\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 4100 / 5267, loss 0.04226, acc 48.05%, total time [00:22], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:15:43.512\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 4200 / 5267, loss 0.04221, acc 50.00%, total time [00:23], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:15:44.052\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 4300 / 5267, loss 0.04202, acc 51.17%, total time [00:23], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:15:44.591\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 4400 / 5267, loss 0.04177, acc 51.95%, total time [00:24], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:15:45.131\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 4500 / 5267, loss 0.04177, acc 50.78%, total time [00:24], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:15:45.670\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 4600 / 5267, loss 0.04195, acc 42.19%, total time [00:25], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:15:46.209\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 4700 / 5267, loss 0.04206, acc 44.53%, total time [00:25], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:15:46.748\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 4800 / 5267, loss 0.04193, acc 51.17%, total time [00:26], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:15:47.366\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 4900 / 5267, loss 0.04160, acc 46.09%, total time [00:27], one epoch time 00:32\u001b[0m\n",
      "\u001b[32m2024-05-10 10:15:47.884\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 5000 / 5267, loss 0.04170, acc 44.53%, total time [00:27], one epoch time 00:27\u001b[0m\n",
      "\u001b[32m2024-05-10 10:15:48.398\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 5100 / 5267, loss 0.04143, acc 46.09%, total time [00:28], one epoch time 00:27\u001b[0m\n",
      "\u001b[32m2024-05-10 10:15:48.926\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 5200 / 5267, loss 0.04132, acc 44.14%, total time [00:28], one epoch time 00:27\u001b[0m\n",
      "\u001b[32m2024-05-10 10:15:49.364\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m118\u001b[0m - \u001b[1mValidation...\u001b[0m\n",
      "100%|██████████| 622/622 [00:01<00:00, 411.99it/s]\n",
      "\u001b[32m2024-05-10 10:15:50.878\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m137\u001b[0m - \u001b[1mEpoch 0: train_loss=0.0538, train_acc=41.62%, val_loss=0.0418, val_acc=47.81%, lr=0.0001\u001b[0m\n",
      "\u001b[32m2024-05-10 10:15:50.879\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m145\u001b[0m - \u001b[1mTesting...\u001b[0m\n",
      "100%|██████████| 5965/5965 [00:14<00:00, 402.02it/s]\n",
      "\u001b[32m2024-05-10 10:16:05.722\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mResults: test_loss=0.0421, test_acc=47.63%\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# set parameters and initiate net\n",
    "use_dynamic_features = False\n",
    "feature_type = 'lmfcc'\n",
    "activate_func = 'relu'\n",
    "batch_size = 256\n",
    "hidden_layer_list = [256, 256, 256, 256]\n",
    "initial_lr = 0.0001\n",
    "num_epochs = 1\n",
    "if feature_type == 'lmfcc':\n",
    "    feat_dim = 13\n",
    "elif feature_type == 'mspec':\n",
    "    feat_dim = 40\n",
    "\n",
    "if use_dynamic_features:\n",
    "    input_size = feat_dim * 7\n",
    "else:\n",
    "    input_size = feat_dim\n",
    "\n",
    "stateList = np.load('state_list.npy').tolist()\n",
    "num_cls = len(stateList)\n",
    "\n",
    "net = Net(input_size=input_size, num_cls=num_cls, hidden_layer_list=hidden_layer_list, activate_func=activate_func)\n",
    "logger.info(net)\n",
    "logger.info('number of prameters:', count_parameters(net))\n",
    "net = net.to(device)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=initial_lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=40, gamma=0.1)\n",
    "\n",
    "# load preprocessed data\n",
    "prepared_data_train = np.load('prepared_train_data.npz')\n",
    "if use_dynamic_features:\n",
    "    np_train_x = prepared_data_train[f'data_x_dynamic_{feature_type}']\n",
    "    np_train_y = prepared_data_train['data_y']\n",
    "else:\n",
    "    np_train_x = prepared_data_train[f'data_x_{feature_type}']\n",
    "    np_train_y = prepared_data_train['data_y']\n",
    "train_x = torch.tensor(np_train_x)\n",
    "train_y = F.one_hot(torch.tensor(np_train_y, dtype=torch.long), num_classes=num_cls).float()\n",
    "\n",
    "prepared_data_val = np.load('prepared_val_data.npz')\n",
    "if use_dynamic_features:\n",
    "    np_val_x = prepared_data_val[f'data_x_dynamic_{feature_type}']\n",
    "    np_val_y = prepared_data_val['data_y']\n",
    "else:\n",
    "    np_val_x = prepared_data_val[f'data_x_{feature_type}']\n",
    "    np_val_y = prepared_data_val['data_y']\n",
    "val_x = torch.tensor(np_val_x)\n",
    "val_y = F.one_hot(torch.tensor(np_val_y, dtype=torch.long), num_classes=num_cls).float()\n",
    "    \n",
    "prepared_data_test = np.load('prepared_test_data.npz')\n",
    "if use_dynamic_features:\n",
    "    np_test_x = prepared_data_test[f'data_x_dynamic_{feature_type}']\n",
    "    np_test_y = prepared_data_test['data_y']\n",
    "else:\n",
    "    np_test_x = prepared_data_test[f'data_x_{feature_type}']\n",
    "    np_test_y = prepared_data_test['data_y']\n",
    "test_x = torch.tensor(np_test_x)\n",
    "test_y = F.one_hot(torch.tensor(np_test_y, dtype=torch.long), num_classes=num_cls).float()\n",
    "\n",
    "# create the data loaders for training and validation sets\n",
    "train_dataset = torch.utils.data.TensorDataset(train_x, train_y)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataset = torch.utils.data.TensorDataset(val_x, val_y)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_dataset = torch.utils.data.TensorDataset(test_x, test_y)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# train the network\n",
    "st = time.time()\n",
    "last_st = time.time()\n",
    "log_interval = 100\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    net.train()\n",
    "    train_loss = 0.0\n",
    "    sub_train_loss = 0.0\n",
    "    epoch_acc = 0.0\n",
    "    cnt = 0\n",
    "    for idx, (inputs, labels) in enumerate(train_loader):\n",
    "\n",
    "        # move data from cpu to gpu (if available)\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # accumulate the training loss\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        sub_train_loss += loss.item()\n",
    "\n",
    "        if idx % log_interval == 0:\n",
    "            duration = time.time() - st\n",
    "            acc = calc_accuracy(labels, outputs)\n",
    "            epoch_acc += (acc * batch_size)\n",
    "            cnt += batch_size\n",
    "            logger.info(f'epoch {epoch:03d} / {num_epochs}, step {idx:04d} / {len(train_loader)}, loss {sub_train_loss / log_interval:.5f}, acc {acc*100:.2f}%, total time [{format_time(duration)}], one epoch time {format_time(len(train_loader) / log_interval * (time.time() - last_st))}')\n",
    "            sub_train_loss = 0.0\n",
    "            last_st = time.time()\n",
    "\n",
    "    # update the learning rate\n",
    "    scheduler.step()\n",
    "\n",
    "    # calculate the validation loss\n",
    "    net.eval()\n",
    "    val_loss = 0.0\n",
    "    val_epoch_acc = 0.0\n",
    "    val_cnt = 0\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0.0\n",
    "        logger.info('Validation...')\n",
    "        for inputs, labels in tqdm(val_loader):\n",
    "            # move data from cpu to gpu (if available)\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            acc = calc_accuracy(labels, outputs)\n",
    "            val_epoch_acc += (acc * batch_size)\n",
    "            val_cnt += len(inputs)\n",
    "\n",
    "    # print the epoch loss\n",
    "    train_loss /= len(train_loader)\n",
    "    val_loss /= len(val_loader)\n",
    "\n",
    "    train_acc = epoch_acc/cnt*100\n",
    "    val_acc = val_epoch_acc/val_cnt*100\n",
    "\n",
    "    logger.info(f'Epoch {epoch}: train_loss={train_loss:.4f}, train_acc={train_acc:.2f}%, val_loss={val_loss:.4f}, val_acc={val_acc:.2f}%, lr={scheduler.get_last_lr()[0]}')\n",
    "    writer.add_scalars('loss',{'train':train_loss,'val':val_loss},epoch)\n",
    "    \n",
    "# finally evaluate model on the test set here\n",
    "net.eval()\n",
    "test_loss = 0.0\n",
    "test_epoch_acc = 0.0\n",
    "test_cnt = 0\n",
    "logger.info('Testing...')\n",
    "with torch.no_grad():\n",
    "    test_loss = 0.0\n",
    "    for inputs, labels in tqdm(test_loader):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_loss += loss.item()\n",
    "        acc = calc_accuracy(labels, outputs)\n",
    "        test_epoch_acc += (acc * batch_size)\n",
    "        test_cnt += len(inputs)\n",
    "test_loss /= len(test_loader)\n",
    "test_acc = test_epoch_acc/test_cnt*100\n",
    "logger.info(f'Results: test_loss={test_loss:.4f}, test_acc={test_acc:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model with dynamic lmfcc with different activate function\n",
    "\n",
    "* dynamic lmfcc\n",
    "* active_func = tanh\n",
    "* batch_size = 256\n",
    "* hidden_layer_list = [256, 256, 256, 256]\n",
    "* initial_lr = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-05-10 10:18:24.399\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m23\u001b[0m - \u001b[1mNet(\n",
      "  (activate): Tanh()\n",
      "  (layer1): Linear(in_features=91, out_features=256, bias=True)\n",
      "  (hidden_layers): Sequential(\n",
      "    (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (1): Tanh()\n",
      "    (2): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (3): Tanh()\n",
      "    (4): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (5): Tanh()\n",
      "  )\n",
      "  (header_layer): Linear(in_features=256, out_features=61, bias=True)\n",
      ")\u001b[0m\n",
      "\u001b[32m2024-05-10 10:18:24.400\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m24\u001b[0m - \u001b[1mnumber of prameters:\u001b[0m\n",
      "\u001b[32m2024-05-10 10:18:25.846\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 0000 / 5267, loss 0.00693, acc 3.12%, total time [00:00], one epoch time 00:04\u001b[0m\n",
      "\u001b[32m2024-05-10 10:18:26.412\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 0100 / 5267, loss 0.41259, acc 1.17%, total time [00:00], one epoch time 00:29\u001b[0m\n",
      "\u001b[32m2024-05-10 10:18:26.960\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 0200 / 5267, loss 0.08312, acc 13.67%, total time [00:01], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:18:27.503\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 0300 / 5267, loss 0.07748, acc 11.33%, total time [00:01], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:18:28.047\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 0400 / 5267, loss 0.07707, acc 11.72%, total time [00:02], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:18:28.668\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 0500 / 5267, loss 0.07631, acc 12.50%, total time [00:02], one epoch time 00:32\u001b[0m\n",
      "\u001b[32m2024-05-10 10:18:29.199\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 0600 / 5267, loss 0.07528, acc 13.67%, total time [00:03], one epoch time 00:27\u001b[0m\n",
      "\u001b[32m2024-05-10 10:18:29.732\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 0700 / 5267, loss 0.07351, acc 13.28%, total time [00:03], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:18:30.275\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 0800 / 5267, loss 0.07179, acc 11.33%, total time [00:04], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:18:30.821\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 0900 / 5267, loss 0.07055, acc 15.62%, total time [00:05], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:18:31.352\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 1000 / 5267, loss 0.06945, acc 14.84%, total time [00:05], one epoch time 00:27\u001b[0m\n",
      "\u001b[32m2024-05-10 10:18:31.896\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 1100 / 5267, loss 0.06842, acc 18.75%, total time [00:06], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:18:32.439\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 1200 / 5267, loss 0.06743, acc 19.53%, total time [00:06], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:18:32.974\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 1300 / 5267, loss 0.06581, acc 21.09%, total time [00:07], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:18:33.505\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 1400 / 5267, loss 0.06398, acc 30.47%, total time [00:07], one epoch time 00:27\u001b[0m\n",
      "\u001b[32m2024-05-10 10:18:34.050\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 1500 / 5267, loss 0.06200, acc 21.09%, total time [00:08], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:18:34.667\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 1600 / 5267, loss 0.06040, acc 29.69%, total time [00:08], one epoch time 00:32\u001b[0m\n",
      "\u001b[32m2024-05-10 10:18:35.198\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 1700 / 5267, loss 0.05899, acc 29.69%, total time [00:09], one epoch time 00:27\u001b[0m\n",
      "\u001b[32m2024-05-10 10:18:35.757\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 1800 / 5267, loss 0.05741, acc 30.86%, total time [00:09], one epoch time 00:29\u001b[0m\n",
      "\u001b[32m2024-05-10 10:18:36.284\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 1900 / 5267, loss 0.05600, acc 33.59%, total time [00:10], one epoch time 00:27\u001b[0m\n",
      "\u001b[32m2024-05-10 10:18:36.805\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 2000 / 5267, loss 0.05498, acc 33.20%, total time [00:11], one epoch time 00:27\u001b[0m\n",
      "\u001b[32m2024-05-10 10:18:37.335\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 2100 / 5267, loss 0.05375, acc 33.98%, total time [00:11], one epoch time 00:27\u001b[0m\n",
      "\u001b[32m2024-05-10 10:18:37.867\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 2200 / 5267, loss 0.05238, acc 41.41%, total time [00:12], one epoch time 00:27\u001b[0m\n",
      "\u001b[32m2024-05-10 10:18:38.409\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 2300 / 5267, loss 0.05152, acc 42.97%, total time [00:12], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:18:38.954\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 2400 / 5267, loss 0.05077, acc 38.28%, total time [00:13], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:18:39.496\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 2500 / 5267, loss 0.04994, acc 38.28%, total time [00:13], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:18:40.041\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 2600 / 5267, loss 0.04901, acc 37.50%, total time [00:14], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:18:40.648\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 2700 / 5267, loss 0.04842, acc 41.80%, total time [00:14], one epoch time 00:31\u001b[0m\n",
      "\u001b[32m2024-05-10 10:18:41.179\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 2800 / 5267, loss 0.04750, acc 44.14%, total time [00:15], one epoch time 00:27\u001b[0m\n",
      "\u001b[32m2024-05-10 10:18:41.708\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 2900 / 5267, loss 0.04676, acc 42.97%, total time [00:15], one epoch time 00:27\u001b[0m\n",
      "\u001b[32m2024-05-10 10:18:42.242\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 3000 / 5267, loss 0.04612, acc 42.58%, total time [00:16], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:18:42.785\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 3100 / 5267, loss 0.04597, acc 46.09%, total time [00:17], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:18:43.327\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 3200 / 5267, loss 0.04527, acc 43.75%, total time [00:17], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:18:43.871\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 3300 / 5267, loss 0.04471, acc 48.44%, total time [00:18], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:18:44.413\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 3400 / 5267, loss 0.04417, acc 47.66%, total time [00:18], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:18:44.955\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 3500 / 5267, loss 0.04359, acc 48.44%, total time [00:19], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:18:45.497\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 3600 / 5267, loss 0.04328, acc 51.95%, total time [00:19], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:18:46.038\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 3700 / 5267, loss 0.04281, acc 48.44%, total time [00:20], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:18:46.633\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 3800 / 5267, loss 0.04240, acc 53.91%, total time [00:20], one epoch time 00:31\u001b[0m\n",
      "\u001b[32m2024-05-10 10:18:47.161\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 3900 / 5267, loss 0.04178, acc 55.47%, total time [00:21], one epoch time 00:27\u001b[0m\n",
      "\u001b[32m2024-05-10 10:18:47.691\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 4000 / 5267, loss 0.04140, acc 53.91%, total time [00:21], one epoch time 00:27\u001b[0m\n",
      "\u001b[32m2024-05-10 10:18:48.222\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 4100 / 5267, loss 0.04107, acc 46.09%, total time [00:22], one epoch time 00:27\u001b[0m\n",
      "\u001b[32m2024-05-10 10:18:48.752\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 4200 / 5267, loss 0.04051, acc 46.09%, total time [00:22], one epoch time 00:27\u001b[0m\n",
      "\u001b[32m2024-05-10 10:18:49.286\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 4300 / 5267, loss 0.04016, acc 53.12%, total time [00:23], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:18:49.841\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 4400 / 5267, loss 0.03968, acc 49.61%, total time [00:24], one epoch time 00:29\u001b[0m\n",
      "\u001b[32m2024-05-10 10:18:50.397\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 4500 / 5267, loss 0.03943, acc 57.81%, total time [00:24], one epoch time 00:29\u001b[0m\n",
      "\u001b[32m2024-05-10 10:18:50.953\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 4600 / 5267, loss 0.03911, acc 53.12%, total time [00:25], one epoch time 00:29\u001b[0m\n",
      "\u001b[32m2024-05-10 10:18:51.509\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 4700 / 5267, loss 0.03871, acc 57.03%, total time [00:25], one epoch time 00:29\u001b[0m\n",
      "\u001b[32m2024-05-10 10:18:52.143\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 4800 / 5267, loss 0.03824, acc 57.03%, total time [00:26], one epoch time 00:33\u001b[0m\n",
      "\u001b[32m2024-05-10 10:18:52.674\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 4900 / 5267, loss 0.03814, acc 57.81%, total time [00:26], one epoch time 00:27\u001b[0m\n",
      "\u001b[32m2024-05-10 10:18:53.216\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 5000 / 5267, loss 0.03784, acc 59.77%, total time [00:27], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:18:53.758\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 5100 / 5267, loss 0.03739, acc 55.08%, total time [00:27], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:18:54.301\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 5200 / 5267, loss 0.03722, acc 56.64%, total time [00:28], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:18:54.755\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m118\u001b[0m - \u001b[1mValidation...\u001b[0m\n",
      "100%|██████████| 622/622 [00:01<00:00, 392.92it/s]\n",
      "\u001b[32m2024-05-10 10:18:56.342\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m137\u001b[0m - \u001b[1mEpoch 0: train_loss=0.0595, train_acc=36.63%, val_loss=0.0376, val_acc=55.14%, lr=0.0001\u001b[0m\n",
      "\u001b[32m2024-05-10 10:18:56.344\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m145\u001b[0m - \u001b[1mTesting...\u001b[0m\n",
      "100%|██████████| 5965/5965 [00:14<00:00, 402.69it/s]\n",
      "\u001b[32m2024-05-10 10:19:11.161\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mResults: test_loss=0.0377, test_acc=54.92%\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# set parameters and initiate net\n",
    "use_dynamic_features = True\n",
    "feature_type = 'lmfcc'\n",
    "activate_func = 'tanh'\n",
    "batch_size = 256\n",
    "hidden_layer_list = [256, 256, 256, 256]\n",
    "initial_lr = 0.0001\n",
    "num_epochs = 1\n",
    "if feature_type == 'lmfcc':\n",
    "    feat_dim = 13\n",
    "elif feature_type == 'mspec':\n",
    "    feat_dim = 40\n",
    "\n",
    "if use_dynamic_features:\n",
    "    input_size = feat_dim * 7\n",
    "else:\n",
    "    input_size = feat_dim\n",
    "\n",
    "stateList = np.load('state_list.npy').tolist()\n",
    "num_cls = len(stateList)\n",
    "\n",
    "net = Net(input_size=input_size, num_cls=num_cls, hidden_layer_list=hidden_layer_list, activate_func=activate_func)\n",
    "logger.info(net)\n",
    "logger.info('number of prameters:', count_parameters(net))\n",
    "net = net.to(device)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=initial_lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=40, gamma=0.1)\n",
    "\n",
    "# load preprocessed data\n",
    "prepared_data_train = np.load('prepared_train_data.npz')\n",
    "if use_dynamic_features:\n",
    "    np_train_x = prepared_data_train[f'data_x_dynamic_{feature_type}']\n",
    "    np_train_y = prepared_data_train['data_y']\n",
    "else:\n",
    "    np_train_x = prepared_data_train[f'data_x_{feature_type}']\n",
    "    np_train_y = prepared_data_train['data_y']\n",
    "train_x = torch.tensor(np_train_x)\n",
    "train_y = F.one_hot(torch.tensor(np_train_y, dtype=torch.long), num_classes=num_cls).float()\n",
    "\n",
    "prepared_data_val = np.load('prepared_val_data.npz')\n",
    "if use_dynamic_features:\n",
    "    np_val_x = prepared_data_val[f'data_x_dynamic_{feature_type}']\n",
    "    np_val_y = prepared_data_val['data_y']\n",
    "else:\n",
    "    np_val_x = prepared_data_val[f'data_x_{feature_type}']\n",
    "    np_val_y = prepared_data_val['data_y']\n",
    "val_x = torch.tensor(np_val_x)\n",
    "val_y = F.one_hot(torch.tensor(np_val_y, dtype=torch.long), num_classes=num_cls).float()\n",
    "    \n",
    "prepared_data_test = np.load('prepared_test_data.npz')\n",
    "if use_dynamic_features:\n",
    "    np_test_x = prepared_data_test[f'data_x_dynamic_{feature_type}']\n",
    "    np_test_y = prepared_data_test['data_y']\n",
    "else:\n",
    "    np_test_x = prepared_data_test[f'data_x_{feature_type}']\n",
    "    np_test_y = prepared_data_test['data_y']\n",
    "test_x = torch.tensor(np_test_x)\n",
    "test_y = F.one_hot(torch.tensor(np_test_y, dtype=torch.long), num_classes=num_cls).float()\n",
    "\n",
    "# create the data loaders for training and validation sets\n",
    "train_dataset = torch.utils.data.TensorDataset(train_x, train_y)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataset = torch.utils.data.TensorDataset(val_x, val_y)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_dataset = torch.utils.data.TensorDataset(test_x, test_y)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# train the network\n",
    "st = time.time()\n",
    "last_st = time.time()\n",
    "log_interval = 100\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    net.train()\n",
    "    train_loss = 0.0\n",
    "    sub_train_loss = 0.0\n",
    "    epoch_acc = 0.0\n",
    "    cnt = 0\n",
    "    for idx, (inputs, labels) in enumerate(train_loader):\n",
    "\n",
    "        # move data from cpu to gpu (if available)\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # accumulate the training loss\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        sub_train_loss += loss.item()\n",
    "\n",
    "        if idx % log_interval == 0:\n",
    "            duration = time.time() - st\n",
    "            acc = calc_accuracy(labels, outputs)\n",
    "            epoch_acc += (acc * batch_size)\n",
    "            cnt += batch_size\n",
    "            logger.info(f'epoch {epoch:03d} / {num_epochs}, step {idx:04d} / {len(train_loader)}, loss {sub_train_loss / log_interval:.5f}, acc {acc*100:.2f}%, total time [{format_time(duration)}], one epoch time {format_time(len(train_loader) / log_interval * (time.time() - last_st))}')\n",
    "            sub_train_loss = 0.0\n",
    "            last_st = time.time()\n",
    "\n",
    "    # update the learning rate\n",
    "    scheduler.step()\n",
    "\n",
    "    # calculate the validation loss\n",
    "    net.eval()\n",
    "    val_loss = 0.0\n",
    "    val_epoch_acc = 0.0\n",
    "    val_cnt = 0\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0.0\n",
    "        logger.info('Validation...')\n",
    "        for inputs, labels in tqdm(val_loader):\n",
    "            # move data from cpu to gpu (if available)\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            acc = calc_accuracy(labels, outputs)\n",
    "            val_epoch_acc += (acc * batch_size)\n",
    "            val_cnt += len(inputs)\n",
    "\n",
    "    # print the epoch loss\n",
    "    train_loss /= len(train_loader)\n",
    "    val_loss /= len(val_loader)\n",
    "\n",
    "    train_acc = epoch_acc/cnt*100\n",
    "    val_acc = val_epoch_acc/val_cnt*100\n",
    "\n",
    "    logger.info(f'Epoch {epoch}: train_loss={train_loss:.4f}, train_acc={train_acc:.2f}%, val_loss={val_loss:.4f}, val_acc={val_acc:.2f}%, lr={scheduler.get_last_lr()[0]}')\n",
    "    writer.add_scalars('loss',{'train':train_loss,'val':val_loss},epoch)\n",
    "    \n",
    "# finally evaluate model on the test set here\n",
    "net.eval()\n",
    "test_loss = 0.0\n",
    "test_epoch_acc = 0.0\n",
    "test_cnt = 0\n",
    "logger.info('Testing...')\n",
    "with torch.no_grad():\n",
    "    test_loss = 0.0\n",
    "    for inputs, labels in tqdm(test_loader):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_loss += loss.item()\n",
    "        acc = calc_accuracy(labels, outputs)\n",
    "        test_epoch_acc += (acc * batch_size)\n",
    "        test_cnt += len(inputs)\n",
    "test_loss /= len(test_loader)\n",
    "test_acc = test_epoch_acc/test_cnt*100\n",
    "logger.info(f'Results: test_loss={test_loss:.4f}, test_acc={test_acc:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model with dynamic lmfcc with different layers\n",
    "\n",
    "* dynamic lmfcc\n",
    "* active_func = relu\n",
    "* batch_size = 256\n",
    "* hidden_layer_list = [256, 256, 256, 256, 256]\n",
    "* initial_lr = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-05-10 10:19:46.929\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m23\u001b[0m - \u001b[1mNet(\n",
      "  (activate): ReLU()\n",
      "  (layer1): Linear(in_features=91, out_features=256, bias=True)\n",
      "  (hidden_layers): Sequential(\n",
      "    (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (7): ReLU()\n",
      "  )\n",
      "  (header_layer): Linear(in_features=256, out_features=61, bias=True)\n",
      ")\u001b[0m\n",
      "\u001b[32m2024-05-10 10:19:46.931\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m24\u001b[0m - \u001b[1mnumber of prameters:\u001b[0m\n",
      "\u001b[32m2024-05-10 10:19:48.401\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 0000 / 5267, loss 0.00693, acc 4.30%, total time [00:00], one epoch time 00:05\u001b[0m\n",
      "\u001b[32m2024-05-10 10:19:49.061\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 0100 / 5267, loss 0.38984, acc 12.50%, total time [00:00], one epoch time 00:34\u001b[0m\n",
      "\u001b[32m2024-05-10 10:19:49.697\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 0200 / 5267, loss 0.08036, acc 10.55%, total time [00:01], one epoch time 00:33\u001b[0m\n",
      "\u001b[32m2024-05-10 10:19:50.335\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 0300 / 5267, loss 0.07429, acc 17.58%, total time [00:02], one epoch time 00:33\u001b[0m\n",
      "\u001b[32m2024-05-10 10:19:51.042\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 0400 / 5267, loss 0.06885, acc 18.36%, total time [00:02], one epoch time 00:37\u001b[0m\n",
      "\u001b[32m2024-05-10 10:19:51.649\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 0500 / 5267, loss 0.06516, acc 19.53%, total time [00:03], one epoch time 00:31\u001b[0m\n",
      "\u001b[32m2024-05-10 10:19:52.278\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 0600 / 5267, loss 0.06214, acc 30.47%, total time [00:03], one epoch time 00:33\u001b[0m\n",
      "\u001b[32m2024-05-10 10:19:52.898\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 0700 / 5267, loss 0.05712, acc 37.11%, total time [00:04], one epoch time 00:32\u001b[0m\n",
      "\u001b[32m2024-05-10 10:19:53.518\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 0800 / 5267, loss 0.05295, acc 35.16%, total time [00:05], one epoch time 00:32\u001b[0m\n",
      "\u001b[32m2024-05-10 10:19:54.136\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 0900 / 5267, loss 0.05033, acc 37.11%, total time [00:05], one epoch time 00:32\u001b[0m\n",
      "\u001b[32m2024-05-10 10:19:54.745\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 1000 / 5267, loss 0.04854, acc 37.50%, total time [00:06], one epoch time 00:32\u001b[0m\n",
      "\u001b[32m2024-05-10 10:19:55.366\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 1100 / 5267, loss 0.04691, acc 44.14%, total time [00:07], one epoch time 00:32\u001b[0m\n",
      "\u001b[32m2024-05-10 10:19:56.001\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 1200 / 5267, loss 0.04581, acc 43.75%, total time [00:07], one epoch time 00:33\u001b[0m\n",
      "\u001b[32m2024-05-10 10:19:56.639\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 1300 / 5267, loss 0.04511, acc 42.58%, total time [00:08], one epoch time 00:33\u001b[0m\n",
      "\u001b[32m2024-05-10 10:19:57.279\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 1400 / 5267, loss 0.04432, acc 49.61%, total time [00:08], one epoch time 00:33\u001b[0m\n",
      "\u001b[32m2024-05-10 10:19:57.945\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 1500 / 5267, loss 0.04332, acc 44.53%, total time [00:09], one epoch time 00:35\u001b[0m\n",
      "\u001b[32m2024-05-10 10:19:58.557\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 1600 / 5267, loss 0.04270, acc 48.44%, total time [00:10], one epoch time 00:32\u001b[0m\n",
      "\u001b[32m2024-05-10 10:19:59.193\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 1700 / 5267, loss 0.04234, acc 53.52%, total time [00:10], one epoch time 00:33\u001b[0m\n",
      "\u001b[32m2024-05-10 10:19:59.830\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 1800 / 5267, loss 0.04150, acc 46.48%, total time [00:11], one epoch time 00:33\u001b[0m\n",
      "\u001b[32m2024-05-10 10:20:00.460\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 1900 / 5267, loss 0.04101, acc 43.75%, total time [00:12], one epoch time 00:33\u001b[0m\n",
      "\u001b[32m2024-05-10 10:20:01.092\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 2000 / 5267, loss 0.04044, acc 46.88%, total time [00:12], one epoch time 00:33\u001b[0m\n",
      "\u001b[32m2024-05-10 10:20:01.728\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 2100 / 5267, loss 0.04013, acc 49.61%, total time [00:13], one epoch time 00:33\u001b[0m\n",
      "\u001b[32m2024-05-10 10:20:02.364\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 2200 / 5267, loss 0.03971, acc 50.00%, total time [00:14], one epoch time 00:33\u001b[0m\n",
      "\u001b[32m2024-05-10 10:20:03.001\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 2300 / 5267, loss 0.03940, acc 50.78%, total time [00:14], one epoch time 00:33\u001b[0m\n",
      "\u001b[32m2024-05-10 10:20:03.636\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 2400 / 5267, loss 0.03899, acc 53.52%, total time [00:15], one epoch time 00:33\u001b[0m\n",
      "\u001b[32m2024-05-10 10:20:04.273\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 2500 / 5267, loss 0.03856, acc 43.75%, total time [00:15], one epoch time 00:33\u001b[0m\n",
      "\u001b[32m2024-05-10 10:20:04.964\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 2600 / 5267, loss 0.03835, acc 55.47%, total time [00:16], one epoch time 00:36\u001b[0m\n",
      "\u001b[32m2024-05-10 10:20:05.584\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 2700 / 5267, loss 0.03782, acc 56.25%, total time [00:17], one epoch time 00:32\u001b[0m\n",
      "\u001b[32m2024-05-10 10:20:06.219\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 2800 / 5267, loss 0.03730, acc 53.52%, total time [00:17], one epoch time 00:33\u001b[0m\n",
      "\u001b[32m2024-05-10 10:20:06.854\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 2900 / 5267, loss 0.03708, acc 55.08%, total time [00:18], one epoch time 00:33\u001b[0m\n",
      "\u001b[32m2024-05-10 10:20:07.491\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 3000 / 5267, loss 0.03690, acc 53.12%, total time [00:19], one epoch time 00:33\u001b[0m\n",
      "\u001b[32m2024-05-10 10:20:08.128\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 3100 / 5267, loss 0.03680, acc 53.52%, total time [00:19], one epoch time 00:33\u001b[0m\n",
      "\u001b[32m2024-05-10 10:20:08.767\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 3200 / 5267, loss 0.03636, acc 54.69%, total time [00:20], one epoch time 00:33\u001b[0m\n",
      "\u001b[32m2024-05-10 10:20:09.403\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 3300 / 5267, loss 0.03611, acc 58.20%, total time [00:21], one epoch time 00:33\u001b[0m\n",
      "\u001b[32m2024-05-10 10:20:10.040\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 3400 / 5267, loss 0.03575, acc 56.25%, total time [00:21], one epoch time 00:33\u001b[0m\n",
      "\u001b[32m2024-05-10 10:20:10.676\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 3500 / 5267, loss 0.03548, acc 57.42%, total time [00:22], one epoch time 00:33\u001b[0m\n",
      "\u001b[32m2024-05-10 10:20:11.314\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 3600 / 5267, loss 0.03545, acc 51.95%, total time [00:23], one epoch time 00:33\u001b[0m\n",
      "\u001b[32m2024-05-10 10:20:11.997\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 3700 / 5267, loss 0.03542, acc 54.30%, total time [00:23], one epoch time 00:35\u001b[0m\n",
      "\u001b[32m2024-05-10 10:20:12.626\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 3800 / 5267, loss 0.03534, acc 54.30%, total time [00:24], one epoch time 00:33\u001b[0m\n",
      "\u001b[32m2024-05-10 10:20:13.246\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 3900 / 5267, loss 0.03510, acc 53.52%, total time [00:24], one epoch time 00:32\u001b[0m\n",
      "\u001b[32m2024-05-10 10:20:13.883\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 4000 / 5267, loss 0.03463, acc 55.08%, total time [00:25], one epoch time 00:33\u001b[0m\n",
      "\u001b[32m2024-05-10 10:20:14.519\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 4100 / 5267, loss 0.03422, acc 59.77%, total time [00:26], one epoch time 00:33\u001b[0m\n",
      "\u001b[32m2024-05-10 10:20:15.155\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 4200 / 5267, loss 0.03417, acc 63.28%, total time [00:26], one epoch time 00:33\u001b[0m\n",
      "\u001b[32m2024-05-10 10:20:15.792\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 4300 / 5267, loss 0.03389, acc 48.05%, total time [00:27], one epoch time 00:33\u001b[0m\n",
      "\u001b[32m2024-05-10 10:20:16.428\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 4400 / 5267, loss 0.03374, acc 61.72%, total time [00:28], one epoch time 00:33\u001b[0m\n",
      "\u001b[32m2024-05-10 10:20:17.065\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 4500 / 5267, loss 0.03374, acc 63.28%, total time [00:28], one epoch time 00:33\u001b[0m\n",
      "\u001b[32m2024-05-10 10:20:17.706\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 4600 / 5267, loss 0.03388, acc 62.50%, total time [00:29], one epoch time 00:33\u001b[0m\n",
      "\u001b[32m2024-05-10 10:20:18.375\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 4700 / 5267, loss 0.03374, acc 60.55%, total time [00:30], one epoch time 00:35\u001b[0m\n",
      "\u001b[32m2024-05-10 10:20:18.983\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 4800 / 5267, loss 0.03322, acc 57.03%, total time [00:30], one epoch time 00:31\u001b[0m\n",
      "\u001b[32m2024-05-10 10:20:19.603\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 4900 / 5267, loss 0.03296, acc 62.50%, total time [00:31], one epoch time 00:32\u001b[0m\n",
      "\u001b[32m2024-05-10 10:20:20.238\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 5000 / 5267, loss 0.03288, acc 60.94%, total time [00:31], one epoch time 00:33\u001b[0m\n",
      "\u001b[32m2024-05-10 10:20:20.874\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 5100 / 5267, loss 0.03314, acc 61.33%, total time [00:32], one epoch time 00:33\u001b[0m\n",
      "\u001b[32m2024-05-10 10:20:21.510\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 5200 / 5267, loss 0.03264, acc 63.67%, total time [00:33], one epoch time 00:33\u001b[0m\n",
      "\u001b[32m2024-05-10 10:20:22.029\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m118\u001b[0m - \u001b[1mValidation...\u001b[0m\n",
      "100%|██████████| 622/622 [00:01<00:00, 380.15it/s]\n",
      "\u001b[32m2024-05-10 10:20:23.669\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m137\u001b[0m - \u001b[1mEpoch 0: train_loss=0.0485, train_acc=47.52%, val_loss=0.0333, val_acc=59.63%, lr=0.0001\u001b[0m\n",
      "\u001b[32m2024-05-10 10:20:23.671\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m145\u001b[0m - \u001b[1mTesting...\u001b[0m\n",
      "100%|██████████| 5965/5965 [00:15<00:00, 387.80it/s]\n",
      "\u001b[32m2024-05-10 10:20:39.058\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mResults: test_loss=0.0338, test_acc=59.01%\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# set parameters and initiate net\n",
    "use_dynamic_features = True\n",
    "feature_type = 'lmfcc'\n",
    "activate_func = 'relu'\n",
    "batch_size = 256\n",
    "hidden_layer_list = [256, 256, 256, 256, 256]\n",
    "initial_lr = 0.0001\n",
    "num_epochs = 1\n",
    "if feature_type == 'lmfcc':\n",
    "    feat_dim = 13\n",
    "elif feature_type == 'mspec':\n",
    "    feat_dim = 40\n",
    "\n",
    "if use_dynamic_features:\n",
    "    input_size = feat_dim * 7\n",
    "else:\n",
    "    input_size = feat_dim\n",
    "\n",
    "stateList = np.load('state_list.npy').tolist()\n",
    "num_cls = len(stateList)\n",
    "\n",
    "net = Net(input_size=input_size, num_cls=num_cls, hidden_layer_list=hidden_layer_list, activate_func=activate_func)\n",
    "logger.info(net)\n",
    "logger.info('number of prameters:', count_parameters(net))\n",
    "net = net.to(device)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=initial_lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=40, gamma=0.1)\n",
    "\n",
    "# load preprocessed data\n",
    "prepared_data_train = np.load('prepared_train_data.npz')\n",
    "if use_dynamic_features:\n",
    "    np_train_x = prepared_data_train[f'data_x_dynamic_{feature_type}']\n",
    "    np_train_y = prepared_data_train['data_y']\n",
    "else:\n",
    "    np_train_x = prepared_data_train[f'data_x_{feature_type}']\n",
    "    np_train_y = prepared_data_train['data_y']\n",
    "train_x = torch.tensor(np_train_x)\n",
    "train_y = F.one_hot(torch.tensor(np_train_y, dtype=torch.long), num_classes=num_cls).float()\n",
    "\n",
    "prepared_data_val = np.load('prepared_val_data.npz')\n",
    "if use_dynamic_features:\n",
    "    np_val_x = prepared_data_val[f'data_x_dynamic_{feature_type}']\n",
    "    np_val_y = prepared_data_val['data_y']\n",
    "else:\n",
    "    np_val_x = prepared_data_val[f'data_x_{feature_type}']\n",
    "    np_val_y = prepared_data_val['data_y']\n",
    "val_x = torch.tensor(np_val_x)\n",
    "val_y = F.one_hot(torch.tensor(np_val_y, dtype=torch.long), num_classes=num_cls).float()\n",
    "    \n",
    "prepared_data_test = np.load('prepared_test_data.npz')\n",
    "if use_dynamic_features:\n",
    "    np_test_x = prepared_data_test[f'data_x_dynamic_{feature_type}']\n",
    "    np_test_y = prepared_data_test['data_y']\n",
    "else:\n",
    "    np_test_x = prepared_data_test[f'data_x_{feature_type}']\n",
    "    np_test_y = prepared_data_test['data_y']\n",
    "test_x = torch.tensor(np_test_x)\n",
    "test_y = F.one_hot(torch.tensor(np_test_y, dtype=torch.long), num_classes=num_cls).float()\n",
    "\n",
    "# create the data loaders for training and validation sets\n",
    "train_dataset = torch.utils.data.TensorDataset(train_x, train_y)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataset = torch.utils.data.TensorDataset(val_x, val_y)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_dataset = torch.utils.data.TensorDataset(test_x, test_y)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# train the network\n",
    "st = time.time()\n",
    "last_st = time.time()\n",
    "log_interval = 100\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    net.train()\n",
    "    train_loss = 0.0\n",
    "    sub_train_loss = 0.0\n",
    "    epoch_acc = 0.0\n",
    "    cnt = 0\n",
    "    for idx, (inputs, labels) in enumerate(train_loader):\n",
    "\n",
    "        # move data from cpu to gpu (if available)\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # accumulate the training loss\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        sub_train_loss += loss.item()\n",
    "\n",
    "        if idx % log_interval == 0:\n",
    "            duration = time.time() - st\n",
    "            acc = calc_accuracy(labels, outputs)\n",
    "            epoch_acc += (acc * batch_size)\n",
    "            cnt += batch_size\n",
    "            logger.info(f'epoch {epoch:03d} / {num_epochs}, step {idx:04d} / {len(train_loader)}, loss {sub_train_loss / log_interval:.5f}, acc {acc*100:.2f}%, total time [{format_time(duration)}], one epoch time {format_time(len(train_loader) / log_interval * (time.time() - last_st))}')\n",
    "            sub_train_loss = 0.0\n",
    "            last_st = time.time()\n",
    "\n",
    "    # update the learning rate\n",
    "    scheduler.step()\n",
    "\n",
    "    # calculate the validation loss\n",
    "    net.eval()\n",
    "    val_loss = 0.0\n",
    "    val_epoch_acc = 0.0\n",
    "    val_cnt = 0\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0.0\n",
    "        logger.info('Validation...')\n",
    "        for inputs, labels in tqdm(val_loader):\n",
    "            # move data from cpu to gpu (if available)\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            acc = calc_accuracy(labels, outputs)\n",
    "            val_epoch_acc += (acc * batch_size)\n",
    "            val_cnt += len(inputs)\n",
    "\n",
    "    # print the epoch loss\n",
    "    train_loss /= len(train_loader)\n",
    "    val_loss /= len(val_loader)\n",
    "\n",
    "    train_acc = epoch_acc/cnt*100\n",
    "    val_acc = val_epoch_acc/val_cnt*100\n",
    "\n",
    "    logger.info(f'Epoch {epoch}: train_loss={train_loss:.4f}, train_acc={train_acc:.2f}%, val_loss={val_loss:.4f}, val_acc={val_acc:.2f}%, lr={scheduler.get_last_lr()[0]}')\n",
    "    writer.add_scalars('loss',{'train':train_loss,'val':val_loss},epoch)\n",
    "    \n",
    "# finally evaluate model on the test set here\n",
    "net.eval()\n",
    "test_loss = 0.0\n",
    "test_epoch_acc = 0.0\n",
    "test_cnt = 0\n",
    "logger.info('Testing...')\n",
    "with torch.no_grad():\n",
    "    test_loss = 0.0\n",
    "    for inputs, labels in tqdm(test_loader):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_loss += loss.item()\n",
    "        acc = calc_accuracy(labels, outputs)\n",
    "        test_epoch_acc += (acc * batch_size)\n",
    "        test_cnt += len(inputs)\n",
    "test_loss /= len(test_loader)\n",
    "test_acc = test_epoch_acc/test_cnt*100\n",
    "logger.info(f'Results: test_loss={test_loss:.4f}, test_acc={test_acc:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model with dynamic lmfcc with different nodes\n",
    "\n",
    "* dynamic lmfcc\n",
    "* active_func = relu\n",
    "* batch_size = 256\n",
    "* hidden_layer_list = [256, 512, 512, 256]\n",
    "* initial_lr = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-05-10 10:24:56.229\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m23\u001b[0m - \u001b[1mNet(\n",
      "  (activate): ReLU()\n",
      "  (layer1): Linear(in_features=91, out_features=256, bias=True)\n",
      "  (hidden_layers): Sequential(\n",
      "    (0): Linear(in_features=256, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (5): ReLU()\n",
      "  )\n",
      "  (header_layer): Linear(in_features=256, out_features=61, bias=True)\n",
      ")\u001b[0m\n",
      "\u001b[32m2024-05-10 10:24:56.230\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m24\u001b[0m - \u001b[1mnumber of prameters:\u001b[0m\n",
      "\u001b[32m2024-05-10 10:24:57.720\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 0000 / 5267, loss 0.00688, acc 1.17%, total time [00:00], one epoch time 00:05\u001b[0m\n",
      "\u001b[32m2024-05-10 10:24:58.305\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 0100 / 5267, loss 0.29893, acc 14.06%, total time [00:00], one epoch time 00:30\u001b[0m\n",
      "\u001b[32m2024-05-10 10:24:58.879\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 0200 / 5267, loss 0.07622, acc 16.80%, total time [00:01], one epoch time 00:30\u001b[0m\n",
      "\u001b[32m2024-05-10 10:24:59.525\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 0300 / 5267, loss 0.06895, acc 21.09%, total time [00:01], one epoch time 00:33\u001b[0m\n",
      "\u001b[32m2024-05-10 10:25:00.072\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 0400 / 5267, loss 0.06227, acc 32.03%, total time [00:02], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:25:00.619\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 0500 / 5267, loss 0.05620, acc 30.08%, total time [00:02], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:25:01.166\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 0600 / 5267, loss 0.05176, acc 41.41%, total time [00:03], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:25:01.713\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 0700 / 5267, loss 0.04848, acc 41.02%, total time [00:04], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:25:02.260\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 0800 / 5267, loss 0.04628, acc 42.97%, total time [00:04], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:25:02.807\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 0900 / 5267, loss 0.04475, acc 45.31%, total time [00:05], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:25:03.354\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 1000 / 5267, loss 0.04403, acc 46.48%, total time [00:05], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:25:03.900\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 1100 / 5267, loss 0.04240, acc 46.48%, total time [00:06], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:25:04.445\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 1200 / 5267, loss 0.04174, acc 41.80%, total time [00:06], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:25:04.991\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 1300 / 5267, loss 0.04078, acc 51.95%, total time [00:07], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:25:05.611\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 1400 / 5267, loss 0.04029, acc 50.78%, total time [00:07], one epoch time 00:32\u001b[0m\n",
      "\u001b[32m2024-05-10 10:25:06.158\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 1500 / 5267, loss 0.03976, acc 56.64%, total time [00:08], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:25:06.704\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 1600 / 5267, loss 0.03936, acc 52.34%, total time [00:09], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:25:07.251\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 1700 / 5267, loss 0.03859, acc 55.47%, total time [00:09], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:25:07.798\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 1800 / 5267, loss 0.03865, acc 53.52%, total time [00:10], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:25:08.345\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 1900 / 5267, loss 0.03780, acc 50.39%, total time [00:10], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:25:08.892\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 2000 / 5267, loss 0.03753, acc 53.91%, total time [00:11], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:25:09.439\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 2100 / 5267, loss 0.03713, acc 58.98%, total time [00:11], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:25:09.987\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 2200 / 5267, loss 0.03681, acc 54.69%, total time [00:12], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:25:10.534\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 2300 / 5267, loss 0.03665, acc 63.28%, total time [00:12], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:25:11.081\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 2400 / 5267, loss 0.03590, acc 56.64%, total time [00:13], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:25:11.694\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 2500 / 5267, loss 0.03604, acc 57.81%, total time [00:14], one epoch time 00:32\u001b[0m\n",
      "\u001b[32m2024-05-10 10:25:12.240\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 2600 / 5267, loss 0.03557, acc 59.38%, total time [00:14], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:25:12.785\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 2700 / 5267, loss 0.03546, acc 57.81%, total time [00:15], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:25:13.330\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 2800 / 5267, loss 0.03537, acc 64.45%, total time [00:15], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:25:13.875\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 2900 / 5267, loss 0.03466, acc 54.30%, total time [00:16], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:25:14.419\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 3000 / 5267, loss 0.03442, acc 57.42%, total time [00:16], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:25:14.963\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 3100 / 5267, loss 0.03429, acc 58.98%, total time [00:17], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:25:15.508\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 3200 / 5267, loss 0.03422, acc 64.45%, total time [00:17], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:25:16.054\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 3300 / 5267, loss 0.03408, acc 63.67%, total time [00:18], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:25:16.599\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 3400 / 5267, loss 0.03395, acc 60.55%, total time [00:18], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:25:17.145\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 3500 / 5267, loss 0.03356, acc 59.38%, total time [00:19], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:25:17.756\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 3600 / 5267, loss 0.03328, acc 60.55%, total time [00:20], one epoch time 00:32\u001b[0m\n",
      "\u001b[32m2024-05-10 10:25:18.304\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 3700 / 5267, loss 0.03303, acc 56.25%, total time [00:20], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:25:18.850\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 3800 / 5267, loss 0.03322, acc 61.72%, total time [00:21], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:25:19.397\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 3900 / 5267, loss 0.03267, acc 61.72%, total time [00:21], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:25:19.944\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 4000 / 5267, loss 0.03258, acc 58.59%, total time [00:22], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:25:20.491\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 4100 / 5267, loss 0.03250, acc 63.67%, total time [00:22], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:25:21.038\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 4200 / 5267, loss 0.03251, acc 64.84%, total time [00:23], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:25:21.586\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 4300 / 5267, loss 0.03245, acc 59.77%, total time [00:23], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:25:22.133\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 4400 / 5267, loss 0.03209, acc 64.84%, total time [00:24], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:25:22.680\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 4500 / 5267, loss 0.03198, acc 63.28%, total time [00:25], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:25:23.305\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 4600 / 5267, loss 0.03159, acc 65.23%, total time [00:25], one epoch time 00:32\u001b[0m\n",
      "\u001b[32m2024-05-10 10:25:23.840\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 4700 / 5267, loss 0.03144, acc 60.16%, total time [00:26], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:25:24.386\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 4800 / 5267, loss 0.03157, acc 63.67%, total time [00:26], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:25:24.932\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 4900 / 5267, loss 0.03172, acc 62.89%, total time [00:27], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:25:25.478\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 5000 / 5267, loss 0.03169, acc 66.80%, total time [00:27], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:25:26.024\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 5100 / 5267, loss 0.03135, acc 61.72%, total time [00:28], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:25:26.570\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 5200 / 5267, loss 0.03132, acc 60.55%, total time [00:28], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:25:27.027\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m118\u001b[0m - \u001b[1mValidation...\u001b[0m\n",
      "100%|██████████| 622/622 [00:01<00:00, 391.19it/s]\n",
      "\u001b[32m2024-05-10 10:25:28.621\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m137\u001b[0m - \u001b[1mEpoch 0: train_loss=0.0436, train_acc=52.71%, val_loss=0.0319, val_acc=61.85%, lr=0.0001\u001b[0m\n",
      "\u001b[32m2024-05-10 10:25:28.623\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m145\u001b[0m - \u001b[1mTesting...\u001b[0m\n",
      "100%|██████████| 5965/5965 [00:14<00:00, 398.00it/s]\n",
      "\u001b[32m2024-05-10 10:25:43.614\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mResults: test_loss=0.0325, test_acc=61.05%\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# set parameters and initiate net\n",
    "use_dynamic_features = True\n",
    "feature_type = 'lmfcc'\n",
    "activate_func = 'relu'\n",
    "batch_size = 256\n",
    "hidden_layer_list = [256, 512, 512, 256]\n",
    "initial_lr = 0.0001\n",
    "num_epochs = 1\n",
    "if feature_type == 'lmfcc':\n",
    "    feat_dim = 13\n",
    "elif feature_type == 'mspec':\n",
    "    feat_dim = 40\n",
    "\n",
    "if use_dynamic_features:\n",
    "    input_size = feat_dim * 7\n",
    "else:\n",
    "    input_size = feat_dim\n",
    "\n",
    "stateList = np.load('state_list.npy').tolist()\n",
    "num_cls = len(stateList)\n",
    "\n",
    "net = Net(input_size=input_size, num_cls=num_cls, hidden_layer_list=hidden_layer_list, activate_func=activate_func)\n",
    "logger.info(net)\n",
    "logger.info('number of prameters:', count_parameters(net))\n",
    "net = net.to(device)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=initial_lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=40, gamma=0.1)\n",
    "\n",
    "# load preprocessed data\n",
    "prepared_data_train = np.load('prepared_train_data.npz')\n",
    "if use_dynamic_features:\n",
    "    np_train_x = prepared_data_train[f'data_x_dynamic_{feature_type}']\n",
    "    np_train_y = prepared_data_train['data_y']\n",
    "else:\n",
    "    np_train_x = prepared_data_train[f'data_x_{feature_type}']\n",
    "    np_train_y = prepared_data_train['data_y']\n",
    "train_x = torch.tensor(np_train_x)\n",
    "train_y = F.one_hot(torch.tensor(np_train_y, dtype=torch.long), num_classes=num_cls).float()\n",
    "\n",
    "prepared_data_val = np.load('prepared_val_data.npz')\n",
    "if use_dynamic_features:\n",
    "    np_val_x = prepared_data_val[f'data_x_dynamic_{feature_type}']\n",
    "    np_val_y = prepared_data_val['data_y']\n",
    "else:\n",
    "    np_val_x = prepared_data_val[f'data_x_{feature_type}']\n",
    "    np_val_y = prepared_data_val['data_y']\n",
    "val_x = torch.tensor(np_val_x)\n",
    "val_y = F.one_hot(torch.tensor(np_val_y, dtype=torch.long), num_classes=num_cls).float()\n",
    "    \n",
    "prepared_data_test = np.load('prepared_test_data.npz')\n",
    "if use_dynamic_features:\n",
    "    np_test_x = prepared_data_test[f'data_x_dynamic_{feature_type}']\n",
    "    np_test_y = prepared_data_test['data_y']\n",
    "else:\n",
    "    np_test_x = prepared_data_test[f'data_x_{feature_type}']\n",
    "    np_test_y = prepared_data_test['data_y']\n",
    "test_x = torch.tensor(np_test_x)\n",
    "test_y = F.one_hot(torch.tensor(np_test_y, dtype=torch.long), num_classes=num_cls).float()\n",
    "\n",
    "# create the data loaders for training and validation sets\n",
    "train_dataset = torch.utils.data.TensorDataset(train_x, train_y)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataset = torch.utils.data.TensorDataset(val_x, val_y)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_dataset = torch.utils.data.TensorDataset(test_x, test_y)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# train the network\n",
    "st = time.time()\n",
    "last_st = time.time()\n",
    "log_interval = 100\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    net.train()\n",
    "    train_loss = 0.0\n",
    "    sub_train_loss = 0.0\n",
    "    epoch_acc = 0.0\n",
    "    cnt = 0\n",
    "    for idx, (inputs, labels) in enumerate(train_loader):\n",
    "\n",
    "        # move data from cpu to gpu (if available)\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # accumulate the training loss\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        sub_train_loss += loss.item()\n",
    "\n",
    "        if idx % log_interval == 0:\n",
    "            duration = time.time() - st\n",
    "            acc = calc_accuracy(labels, outputs)\n",
    "            epoch_acc += (acc * batch_size)\n",
    "            cnt += batch_size\n",
    "            logger.info(f'epoch {epoch:03d} / {num_epochs}, step {idx:04d} / {len(train_loader)}, loss {sub_train_loss / log_interval:.5f}, acc {acc*100:.2f}%, total time [{format_time(duration)}], one epoch time {format_time(len(train_loader) / log_interval * (time.time() - last_st))}')\n",
    "            sub_train_loss = 0.0\n",
    "            last_st = time.time()\n",
    "\n",
    "    # update the learning rate\n",
    "    scheduler.step()\n",
    "\n",
    "    # calculate the validation loss\n",
    "    net.eval()\n",
    "    val_loss = 0.0\n",
    "    val_epoch_acc = 0.0\n",
    "    val_cnt = 0\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0.0\n",
    "        logger.info('Validation...')\n",
    "        for inputs, labels in tqdm(val_loader):\n",
    "            # move data from cpu to gpu (if available)\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            acc = calc_accuracy(labels, outputs)\n",
    "            val_epoch_acc += (acc * batch_size)\n",
    "            val_cnt += len(inputs)\n",
    "\n",
    "    # print the epoch loss\n",
    "    train_loss /= len(train_loader)\n",
    "    val_loss /= len(val_loader)\n",
    "\n",
    "    train_acc = epoch_acc/cnt*100\n",
    "    val_acc = val_epoch_acc/val_cnt*100\n",
    "\n",
    "    logger.info(f'Epoch {epoch}: train_loss={train_loss:.4f}, train_acc={train_acc:.2f}%, val_loss={val_loss:.4f}, val_acc={val_acc:.2f}%, lr={scheduler.get_last_lr()[0]}')\n",
    "    writer.add_scalars('loss',{'train':train_loss,'val':val_loss},epoch)\n",
    "    \n",
    "# finally evaluate model on the test set here\n",
    "net.eval()\n",
    "test_loss = 0.0\n",
    "test_epoch_acc = 0.0\n",
    "test_cnt = 0\n",
    "logger.info('Testing...')\n",
    "with torch.no_grad():\n",
    "    test_loss = 0.0\n",
    "    for inputs, labels in tqdm(test_loader):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_loss += loss.item()\n",
    "        acc = calc_accuracy(labels, outputs)\n",
    "        test_epoch_acc += (acc * batch_size)\n",
    "        test_cnt += len(inputs)\n",
    "test_loss /= len(test_loader)\n",
    "test_acc = test_epoch_acc/test_cnt*100\n",
    "logger.info(f'Results: test_loss={test_loss:.4f}, test_acc={test_acc:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model with dynamic lmfcc with different nodes\n",
    "\n",
    "* dynamic lmfcc\n",
    "* active_func = relu\n",
    "* batch_size = 256\n",
    "* hidden_layer_list = [128, 128, 128, 128]\n",
    "* initial_lr = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-05-10 10:26:33.284\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m23\u001b[0m - \u001b[1mNet(\n",
      "  (activate): ReLU()\n",
      "  (layer1): Linear(in_features=91, out_features=128, bias=True)\n",
      "  (hidden_layers): Sequential(\n",
      "    (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (5): ReLU()\n",
      "  )\n",
      "  (header_layer): Linear(in_features=128, out_features=61, bias=True)\n",
      ")\u001b[0m\n",
      "\u001b[32m2024-05-10 10:26:33.285\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m24\u001b[0m - \u001b[1mnumber of prameters:\u001b[0m\n",
      "\u001b[32m2024-05-10 10:26:34.797\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 0000 / 5267, loss 0.00691, acc 0.39%, total time [00:00], one epoch time 00:05\u001b[0m\n",
      "\u001b[32m2024-05-10 10:26:35.300\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 0100 / 5267, loss 0.59504, acc 1.17%, total time [00:00], one epoch time 00:26\u001b[0m\n",
      "\u001b[32m2024-05-10 10:26:35.835\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 0200 / 5267, loss 0.12221, acc 12.50%, total time [00:01], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:26:36.289\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 0300 / 5267, loss 0.08263, acc 12.11%, total time [00:01], one epoch time 00:23\u001b[0m\n",
      "\u001b[32m2024-05-10 10:26:36.744\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 0400 / 5267, loss 0.07685, acc 12.89%, total time [00:02], one epoch time 00:23\u001b[0m\n",
      "\u001b[32m2024-05-10 10:26:37.198\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 0500 / 5267, loss 0.07297, acc 19.92%, total time [00:02], one epoch time 00:23\u001b[0m\n",
      "\u001b[32m2024-05-10 10:26:37.653\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 0600 / 5267, loss 0.07005, acc 21.88%, total time [00:02], one epoch time 00:23\u001b[0m\n",
      "\u001b[32m2024-05-10 10:26:38.107\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 0700 / 5267, loss 0.06736, acc 23.05%, total time [00:03], one epoch time 00:23\u001b[0m\n",
      "\u001b[32m2024-05-10 10:26:38.591\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 0800 / 5267, loss 0.06516, acc 22.27%, total time [00:03], one epoch time 00:25\u001b[0m\n",
      "\u001b[32m2024-05-10 10:26:39.115\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 0900 / 5267, loss 0.06213, acc 24.22%, total time [00:04], one epoch time 00:27\u001b[0m\n",
      "\u001b[32m2024-05-10 10:26:39.658\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 1000 / 5267, loss 0.05976, acc 27.34%, total time [00:04], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:26:40.203\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 1100 / 5267, loss 0.05728, acc 25.39%, total time [00:05], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:26:40.747\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 1200 / 5267, loss 0.05518, acc 35.55%, total time [00:06], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:26:41.359\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 1300 / 5267, loss 0.05323, acc 37.89%, total time [00:06], one epoch time 00:32\u001b[0m\n",
      "\u001b[32m2024-05-10 10:26:41.890\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 1400 / 5267, loss 0.05138, acc 44.53%, total time [00:07], one epoch time 00:27\u001b[0m\n",
      "\u001b[32m2024-05-10 10:26:42.422\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 1500 / 5267, loss 0.05057, acc 37.50%, total time [00:07], one epoch time 00:27\u001b[0m\n",
      "\u001b[32m2024-05-10 10:26:42.967\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 1600 / 5267, loss 0.04937, acc 44.14%, total time [00:08], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:26:43.510\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 1700 / 5267, loss 0.04815, acc 45.31%, total time [00:08], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:26:44.055\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 1800 / 5267, loss 0.04746, acc 49.61%, total time [00:09], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:26:44.600\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 1900 / 5267, loss 0.04686, acc 44.92%, total time [00:09], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:26:45.145\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 2000 / 5267, loss 0.04641, acc 42.58%, total time [00:10], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:26:45.690\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 2100 / 5267, loss 0.04559, acc 38.67%, total time [00:10], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:26:46.235\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 2200 / 5267, loss 0.04504, acc 44.92%, total time [00:11], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:26:46.781\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 2300 / 5267, loss 0.04450, acc 45.70%, total time [00:12], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:26:47.385\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 2400 / 5267, loss 0.04418, acc 45.31%, total time [00:12], one epoch time 00:31\u001b[0m\n",
      "\u001b[32m2024-05-10 10:26:47.915\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 2500 / 5267, loss 0.04367, acc 46.48%, total time [00:13], one epoch time 00:27\u001b[0m\n",
      "\u001b[32m2024-05-10 10:26:48.445\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 2600 / 5267, loss 0.04326, acc 44.14%, total time [00:13], one epoch time 00:27\u001b[0m\n",
      "\u001b[32m2024-05-10 10:26:48.976\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 2700 / 5267, loss 0.04294, acc 41.02%, total time [00:14], one epoch time 00:27\u001b[0m\n",
      "\u001b[32m2024-05-10 10:26:49.506\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 2800 / 5267, loss 0.04248, acc 44.53%, total time [00:14], one epoch time 00:27\u001b[0m\n",
      "\u001b[32m2024-05-10 10:26:50.037\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 2900 / 5267, loss 0.04224, acc 48.44%, total time [00:15], one epoch time 00:27\u001b[0m\n",
      "\u001b[32m2024-05-10 10:26:50.568\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 3000 / 5267, loss 0.04189, acc 46.09%, total time [00:15], one epoch time 00:27\u001b[0m\n",
      "\u001b[32m2024-05-10 10:26:51.098\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 3100 / 5267, loss 0.04201, acc 46.88%, total time [00:16], one epoch time 00:27\u001b[0m\n",
      "\u001b[32m2024-05-10 10:26:51.629\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 3200 / 5267, loss 0.04152, acc 48.83%, total time [00:16], one epoch time 00:27\u001b[0m\n",
      "\u001b[32m2024-05-10 10:26:52.159\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 3300 / 5267, loss 0.04127, acc 48.44%, total time [00:17], one epoch time 00:27\u001b[0m\n",
      "\u001b[32m2024-05-10 10:26:52.690\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 3400 / 5267, loss 0.04119, acc 51.95%, total time [00:17], one epoch time 00:27\u001b[0m\n",
      "\u001b[32m2024-05-10 10:26:53.286\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 3500 / 5267, loss 0.04093, acc 53.12%, total time [00:18], one epoch time 00:31\u001b[0m\n",
      "\u001b[32m2024-05-10 10:26:53.817\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 3600 / 5267, loss 0.04026, acc 51.56%, total time [00:19], one epoch time 00:27\u001b[0m\n",
      "\u001b[32m2024-05-10 10:26:54.349\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 3700 / 5267, loss 0.04017, acc 40.62%, total time [00:19], one epoch time 00:27\u001b[0m\n",
      "\u001b[32m2024-05-10 10:26:54.879\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 3800 / 5267, loss 0.04011, acc 56.64%, total time [00:20], one epoch time 00:27\u001b[0m\n",
      "\u001b[32m2024-05-10 10:26:55.410\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 3900 / 5267, loss 0.03974, acc 53.91%, total time [00:20], one epoch time 00:27\u001b[0m\n",
      "\u001b[32m2024-05-10 10:26:55.941\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 4000 / 5267, loss 0.03953, acc 55.47%, total time [00:21], one epoch time 00:27\u001b[0m\n",
      "\u001b[32m2024-05-10 10:26:56.471\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 4100 / 5267, loss 0.03966, acc 50.00%, total time [00:21], one epoch time 00:27\u001b[0m\n",
      "\u001b[32m2024-05-10 10:26:57.002\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 4200 / 5267, loss 0.03906, acc 49.61%, total time [00:22], one epoch time 00:27\u001b[0m\n",
      "\u001b[32m2024-05-10 10:26:57.533\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 4300 / 5267, loss 0.03904, acc 53.91%, total time [00:22], one epoch time 00:27\u001b[0m\n",
      "\u001b[32m2024-05-10 10:26:58.064\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 4400 / 5267, loss 0.03891, acc 55.08%, total time [00:23], one epoch time 00:27\u001b[0m\n",
      "\u001b[32m2024-05-10 10:26:58.671\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 4500 / 5267, loss 0.03856, acc 56.64%, total time [00:23], one epoch time 00:31\u001b[0m\n",
      "\u001b[32m2024-05-10 10:26:59.190\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 4600 / 5267, loss 0.03838, acc 58.59%, total time [00:24], one epoch time 00:27\u001b[0m\n",
      "\u001b[32m2024-05-10 10:26:59.720\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 4700 / 5267, loss 0.03819, acc 56.25%, total time [00:25], one epoch time 00:27\u001b[0m\n",
      "\u001b[32m2024-05-10 10:27:00.250\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 4800 / 5267, loss 0.03813, acc 55.47%, total time [00:25], one epoch time 00:27\u001b[0m\n",
      "\u001b[32m2024-05-10 10:27:00.781\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 4900 / 5267, loss 0.03788, acc 53.52%, total time [00:26], one epoch time 00:27\u001b[0m\n",
      "\u001b[32m2024-05-10 10:27:01.312\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 5000 / 5267, loss 0.03758, acc 56.64%, total time [00:26], one epoch time 00:27\u001b[0m\n",
      "\u001b[32m2024-05-10 10:27:01.855\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 5100 / 5267, loss 0.03775, acc 58.59%, total time [00:27], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:27:02.399\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 5200 / 5267, loss 0.03706, acc 61.72%, total time [00:27], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:27:02.855\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m118\u001b[0m - \u001b[1mValidation...\u001b[0m\n",
      "100%|██████████| 622/622 [00:01<00:00, 405.50it/s]\n",
      "\u001b[32m2024-05-10 10:27:04.393\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m137\u001b[0m - \u001b[1mEpoch 0: train_loss=0.0591, train_acc=41.58%, val_loss=0.0377, val_acc=54.22%, lr=0.0001\u001b[0m\n",
      "\u001b[32m2024-05-10 10:27:04.395\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m145\u001b[0m - \u001b[1mTesting...\u001b[0m\n",
      "100%|██████████| 5965/5965 [00:14<00:00, 414.21it/s]\n",
      "\u001b[32m2024-05-10 10:27:18.800\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mResults: test_loss=0.0381, test_acc=53.61%\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# set parameters and initiate net\n",
    "use_dynamic_features = True\n",
    "feature_type = 'lmfcc'\n",
    "activate_func = 'relu'\n",
    "batch_size = 256\n",
    "hidden_layer_list = [128, 128, 128, 128]\n",
    "initial_lr = 0.0001\n",
    "num_epochs = 1\n",
    "if feature_type == 'lmfcc':\n",
    "    feat_dim = 13\n",
    "elif feature_type == 'mspec':\n",
    "    feat_dim = 40\n",
    "\n",
    "if use_dynamic_features:\n",
    "    input_size = feat_dim * 7\n",
    "else:\n",
    "    input_size = feat_dim\n",
    "\n",
    "stateList = np.load('state_list.npy').tolist()\n",
    "num_cls = len(stateList)\n",
    "\n",
    "net = Net(input_size=input_size, num_cls=num_cls, hidden_layer_list=hidden_layer_list, activate_func=activate_func)\n",
    "logger.info(net)\n",
    "logger.info('number of prameters:', count_parameters(net))\n",
    "net = net.to(device)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=initial_lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=40, gamma=0.1)\n",
    "\n",
    "# load preprocessed data\n",
    "prepared_data_train = np.load('prepared_train_data.npz')\n",
    "if use_dynamic_features:\n",
    "    np_train_x = prepared_data_train[f'data_x_dynamic_{feature_type}']\n",
    "    np_train_y = prepared_data_train['data_y']\n",
    "else:\n",
    "    np_train_x = prepared_data_train[f'data_x_{feature_type}']\n",
    "    np_train_y = prepared_data_train['data_y']\n",
    "train_x = torch.tensor(np_train_x)\n",
    "train_y = F.one_hot(torch.tensor(np_train_y, dtype=torch.long), num_classes=num_cls).float()\n",
    "\n",
    "prepared_data_val = np.load('prepared_val_data.npz')\n",
    "if use_dynamic_features:\n",
    "    np_val_x = prepared_data_val[f'data_x_dynamic_{feature_type}']\n",
    "    np_val_y = prepared_data_val['data_y']\n",
    "else:\n",
    "    np_val_x = prepared_data_val[f'data_x_{feature_type}']\n",
    "    np_val_y = prepared_data_val['data_y']\n",
    "val_x = torch.tensor(np_val_x)\n",
    "val_y = F.one_hot(torch.tensor(np_val_y, dtype=torch.long), num_classes=num_cls).float()\n",
    "    \n",
    "prepared_data_test = np.load('prepared_test_data.npz')\n",
    "if use_dynamic_features:\n",
    "    np_test_x = prepared_data_test[f'data_x_dynamic_{feature_type}']\n",
    "    np_test_y = prepared_data_test['data_y']\n",
    "else:\n",
    "    np_test_x = prepared_data_test[f'data_x_{feature_type}']\n",
    "    np_test_y = prepared_data_test['data_y']\n",
    "test_x = torch.tensor(np_test_x)\n",
    "test_y = F.one_hot(torch.tensor(np_test_y, dtype=torch.long), num_classes=num_cls).float()\n",
    "\n",
    "# create the data loaders for training and validation sets\n",
    "train_dataset = torch.utils.data.TensorDataset(train_x, train_y)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataset = torch.utils.data.TensorDataset(val_x, val_y)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_dataset = torch.utils.data.TensorDataset(test_x, test_y)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# train the network\n",
    "st = time.time()\n",
    "last_st = time.time()\n",
    "log_interval = 100\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    net.train()\n",
    "    train_loss = 0.0\n",
    "    sub_train_loss = 0.0\n",
    "    epoch_acc = 0.0\n",
    "    cnt = 0\n",
    "    for idx, (inputs, labels) in enumerate(train_loader):\n",
    "\n",
    "        # move data from cpu to gpu (if available)\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # accumulate the training loss\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        sub_train_loss += loss.item()\n",
    "\n",
    "        if idx % log_interval == 0:\n",
    "            duration = time.time() - st\n",
    "            acc = calc_accuracy(labels, outputs)\n",
    "            epoch_acc += (acc * batch_size)\n",
    "            cnt += batch_size\n",
    "            logger.info(f'epoch {epoch:03d} / {num_epochs}, step {idx:04d} / {len(train_loader)}, loss {sub_train_loss / log_interval:.5f}, acc {acc*100:.2f}%, total time [{format_time(duration)}], one epoch time {format_time(len(train_loader) / log_interval * (time.time() - last_st))}')\n",
    "            sub_train_loss = 0.0\n",
    "            last_st = time.time()\n",
    "\n",
    "    # update the learning rate\n",
    "    scheduler.step()\n",
    "\n",
    "    # calculate the validation loss\n",
    "    net.eval()\n",
    "    val_loss = 0.0\n",
    "    val_epoch_acc = 0.0\n",
    "    val_cnt = 0\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0.0\n",
    "        logger.info('Validation...')\n",
    "        for inputs, labels in tqdm(val_loader):\n",
    "            # move data from cpu to gpu (if available)\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            acc = calc_accuracy(labels, outputs)\n",
    "            val_epoch_acc += (acc * batch_size)\n",
    "            val_cnt += len(inputs)\n",
    "\n",
    "    # print the epoch loss\n",
    "    train_loss /= len(train_loader)\n",
    "    val_loss /= len(val_loader)\n",
    "\n",
    "    train_acc = epoch_acc/cnt*100\n",
    "    val_acc = val_epoch_acc/val_cnt*100\n",
    "\n",
    "    logger.info(f'Epoch {epoch}: train_loss={train_loss:.4f}, train_acc={train_acc:.2f}%, val_loss={val_loss:.4f}, val_acc={val_acc:.2f}%, lr={scheduler.get_last_lr()[0]}')\n",
    "    writer.add_scalars('loss',{'train':train_loss,'val':val_loss},epoch)\n",
    "    \n",
    "# finally evaluate model on the test set here\n",
    "net.eval()\n",
    "test_loss = 0.0\n",
    "test_epoch_acc = 0.0\n",
    "test_cnt = 0\n",
    "logger.info('Testing...')\n",
    "with torch.no_grad():\n",
    "    test_loss = 0.0\n",
    "    for inputs, labels in tqdm(test_loader):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_loss += loss.item()\n",
    "        acc = calc_accuracy(labels, outputs)\n",
    "        test_epoch_acc += (acc * batch_size)\n",
    "        test_cnt += len(inputs)\n",
    "test_loss /= len(test_loader)\n",
    "test_acc = test_epoch_acc/test_cnt*100\n",
    "logger.info(f'Results: test_loss={test_loss:.4f}, test_acc={test_acc:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model with dynamic lmfcc with different batch size\n",
    "\n",
    "* dynamic lmfcc\n",
    "* active_func = relu\n",
    "* batch_size = 512\n",
    "* hidden_layer_list = [256, 256, 256, 256]\n",
    "* initial_lr = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-05-10 10:28:04.586\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m23\u001b[0m - \u001b[1mNet(\n",
      "  (activate): ReLU()\n",
      "  (layer1): Linear(in_features=91, out_features=256, bias=True)\n",
      "  (hidden_layers): Sequential(\n",
      "    (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (5): ReLU()\n",
      "  )\n",
      "  (header_layer): Linear(in_features=256, out_features=61, bias=True)\n",
      ")\u001b[0m\n",
      "\u001b[32m2024-05-10 10:28:04.588\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m24\u001b[0m - \u001b[1mnumber of prameters:\u001b[0m\n",
      "\u001b[32m2024-05-10 10:28:06.010\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 0000 / 2634, loss 0.00693, acc 1.56%, total time [00:00], one epoch time 00:02\u001b[0m\n",
      "\u001b[32m2024-05-10 10:28:06.754\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 0100 / 2634, loss 0.39057, acc 12.89%, total time [00:00], one epoch time 00:19\u001b[0m\n",
      "\u001b[32m2024-05-10 10:28:07.408\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 0200 / 2634, loss 0.08045, acc 11.33%, total time [00:01], one epoch time 00:17\u001b[0m\n",
      "\u001b[32m2024-05-10 10:28:08.062\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 0300 / 2634, loss 0.07215, acc 24.02%, total time [00:02], one epoch time 00:17\u001b[0m\n",
      "\u001b[32m2024-05-10 10:28:08.796\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 0400 / 2634, loss 0.06612, acc 25.78%, total time [00:02], one epoch time 00:19\u001b[0m\n",
      "\u001b[32m2024-05-10 10:28:09.451\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 0500 / 2634, loss 0.06099, acc 29.69%, total time [00:03], one epoch time 00:17\u001b[0m\n",
      "\u001b[32m2024-05-10 10:28:10.104\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 0600 / 2634, loss 0.05638, acc 38.28%, total time [00:04], one epoch time 00:17\u001b[0m\n",
      "\u001b[32m2024-05-10 10:28:10.838\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 0700 / 2634, loss 0.05197, acc 40.23%, total time [00:04], one epoch time 00:19\u001b[0m\n",
      "\u001b[32m2024-05-10 10:28:11.493\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 0800 / 2634, loss 0.04907, acc 35.74%, total time [00:05], one epoch time 00:17\u001b[0m\n",
      "\u001b[32m2024-05-10 10:28:12.148\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 0900 / 2634, loss 0.04702, acc 40.43%, total time [00:06], one epoch time 00:17\u001b[0m\n",
      "\u001b[32m2024-05-10 10:28:12.880\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 1000 / 2634, loss 0.04531, acc 43.36%, total time [00:06], one epoch time 00:19\u001b[0m\n",
      "\u001b[32m2024-05-10 10:28:13.534\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 1100 / 2634, loss 0.04421, acc 46.88%, total time [00:07], one epoch time 00:17\u001b[0m\n",
      "\u001b[32m2024-05-10 10:28:14.188\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 1200 / 2634, loss 0.04342, acc 44.92%, total time [00:08], one epoch time 00:17\u001b[0m\n",
      "\u001b[32m2024-05-10 10:28:14.922\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 1300 / 2634, loss 0.04239, acc 49.61%, total time [00:09], one epoch time 00:19\u001b[0m\n",
      "\u001b[32m2024-05-10 10:28:15.577\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 1400 / 2634, loss 0.04170, acc 46.29%, total time [00:09], one epoch time 00:17\u001b[0m\n",
      "\u001b[32m2024-05-10 10:28:16.231\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 1500 / 2634, loss 0.04147, acc 49.61%, total time [00:10], one epoch time 00:17\u001b[0m\n",
      "\u001b[32m2024-05-10 10:28:16.965\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 1600 / 2634, loss 0.04055, acc 50.98%, total time [00:11], one epoch time 00:19\u001b[0m\n",
      "\u001b[32m2024-05-10 10:28:17.621\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 1700 / 2634, loss 0.03977, acc 52.54%, total time [00:11], one epoch time 00:17\u001b[0m\n",
      "\u001b[32m2024-05-10 10:28:18.276\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 1800 / 2634, loss 0.03947, acc 54.69%, total time [00:12], one epoch time 00:17\u001b[0m\n",
      "\u001b[32m2024-05-10 10:28:19.012\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 1900 / 2634, loss 0.03887, acc 52.15%, total time [00:13], one epoch time 00:19\u001b[0m\n",
      "\u001b[32m2024-05-10 10:28:19.667\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 2000 / 2634, loss 0.03873, acc 54.10%, total time [00:13], one epoch time 00:17\u001b[0m\n",
      "\u001b[32m2024-05-10 10:28:20.322\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 2100 / 2634, loss 0.03800, acc 53.71%, total time [00:14], one epoch time 00:17\u001b[0m\n",
      "\u001b[32m2024-05-10 10:28:21.057\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 2200 / 2634, loss 0.03749, acc 55.47%, total time [00:15], one epoch time 00:19\u001b[0m\n",
      "\u001b[32m2024-05-10 10:28:21.705\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 2300 / 2634, loss 0.03723, acc 54.69%, total time [00:15], one epoch time 00:17\u001b[0m\n",
      "\u001b[32m2024-05-10 10:28:22.361\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 2400 / 2634, loss 0.03688, acc 53.32%, total time [00:16], one epoch time 00:17\u001b[0m\n",
      "\u001b[32m2024-05-10 10:28:23.097\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 2500 / 2634, loss 0.03650, acc 58.01%, total time [00:17], one epoch time 00:19\u001b[0m\n",
      "\u001b[32m2024-05-10 10:28:23.753\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 2600 / 2634, loss 0.03636, acc 60.55%, total time [00:17], one epoch time 00:17\u001b[0m\n",
      "\u001b[32m2024-05-10 10:28:24.063\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m118\u001b[0m - \u001b[1mValidation...\u001b[0m\n",
      "100%|██████████| 311/311 [00:01<00:00, 251.64it/s]\n",
      "\u001b[32m2024-05-10 10:28:25.303\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m137\u001b[0m - \u001b[1mEpoch 0: train_loss=0.0597, train_acc=42.25%, val_loss=0.0365, val_acc=55.65%, lr=0.0001\u001b[0m\n",
      "\u001b[32m2024-05-10 10:28:25.304\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m145\u001b[0m - \u001b[1mTesting...\u001b[0m\n",
      "100%|██████████| 2983/2983 [00:11<00:00, 256.84it/s]\n",
      "\u001b[32m2024-05-10 10:28:36.923\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mResults: test_loss=0.0369, test_acc=55.27%\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# set parameters and initiate net\n",
    "use_dynamic_features = True\n",
    "feature_type = 'lmfcc'\n",
    "activate_func = 'relu'\n",
    "batch_size = 512\n",
    "hidden_layer_list = [256, 256, 256, 256]\n",
    "initial_lr = 0.0001\n",
    "num_epochs = 1\n",
    "if feature_type == 'lmfcc':\n",
    "    feat_dim = 13\n",
    "elif feature_type == 'mspec':\n",
    "    feat_dim = 40\n",
    "\n",
    "if use_dynamic_features:\n",
    "    input_size = feat_dim * 7\n",
    "else:\n",
    "    input_size = feat_dim\n",
    "\n",
    "stateList = np.load('state_list.npy').tolist()\n",
    "num_cls = len(stateList)\n",
    "\n",
    "net = Net(input_size=input_size, num_cls=num_cls, hidden_layer_list=hidden_layer_list, activate_func=activate_func)\n",
    "logger.info(net)\n",
    "logger.info('number of prameters:', count_parameters(net))\n",
    "net = net.to(device)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=initial_lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=40, gamma=0.1)\n",
    "\n",
    "# load preprocessed data\n",
    "prepared_data_train = np.load('prepared_train_data.npz')\n",
    "if use_dynamic_features:\n",
    "    np_train_x = prepared_data_train[f'data_x_dynamic_{feature_type}']\n",
    "    np_train_y = prepared_data_train['data_y']\n",
    "else:\n",
    "    np_train_x = prepared_data_train[f'data_x_{feature_type}']\n",
    "    np_train_y = prepared_data_train['data_y']\n",
    "train_x = torch.tensor(np_train_x)\n",
    "train_y = F.one_hot(torch.tensor(np_train_y, dtype=torch.long), num_classes=num_cls).float()\n",
    "\n",
    "prepared_data_val = np.load('prepared_val_data.npz')\n",
    "if use_dynamic_features:\n",
    "    np_val_x = prepared_data_val[f'data_x_dynamic_{feature_type}']\n",
    "    np_val_y = prepared_data_val['data_y']\n",
    "else:\n",
    "    np_val_x = prepared_data_val[f'data_x_{feature_type}']\n",
    "    np_val_y = prepared_data_val['data_y']\n",
    "val_x = torch.tensor(np_val_x)\n",
    "val_y = F.one_hot(torch.tensor(np_val_y, dtype=torch.long), num_classes=num_cls).float()\n",
    "    \n",
    "prepared_data_test = np.load('prepared_test_data.npz')\n",
    "if use_dynamic_features:\n",
    "    np_test_x = prepared_data_test[f'data_x_dynamic_{feature_type}']\n",
    "    np_test_y = prepared_data_test['data_y']\n",
    "else:\n",
    "    np_test_x = prepared_data_test[f'data_x_{feature_type}']\n",
    "    np_test_y = prepared_data_test['data_y']\n",
    "test_x = torch.tensor(np_test_x)\n",
    "test_y = F.one_hot(torch.tensor(np_test_y, dtype=torch.long), num_classes=num_cls).float()\n",
    "\n",
    "# create the data loaders for training and validation sets\n",
    "train_dataset = torch.utils.data.TensorDataset(train_x, train_y)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataset = torch.utils.data.TensorDataset(val_x, val_y)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_dataset = torch.utils.data.TensorDataset(test_x, test_y)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# train the network\n",
    "st = time.time()\n",
    "last_st = time.time()\n",
    "log_interval = 100\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    net.train()\n",
    "    train_loss = 0.0\n",
    "    sub_train_loss = 0.0\n",
    "    epoch_acc = 0.0\n",
    "    cnt = 0\n",
    "    for idx, (inputs, labels) in enumerate(train_loader):\n",
    "\n",
    "        # move data from cpu to gpu (if available)\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # accumulate the training loss\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        sub_train_loss += loss.item()\n",
    "\n",
    "        if idx % log_interval == 0:\n",
    "            duration = time.time() - st\n",
    "            acc = calc_accuracy(labels, outputs)\n",
    "            epoch_acc += (acc * batch_size)\n",
    "            cnt += batch_size\n",
    "            logger.info(f'epoch {epoch:03d} / {num_epochs}, step {idx:04d} / {len(train_loader)}, loss {sub_train_loss / log_interval:.5f}, acc {acc*100:.2f}%, total time [{format_time(duration)}], one epoch time {format_time(len(train_loader) / log_interval * (time.time() - last_st))}')\n",
    "            sub_train_loss = 0.0\n",
    "            last_st = time.time()\n",
    "\n",
    "    # update the learning rate\n",
    "    scheduler.step()\n",
    "\n",
    "    # calculate the validation loss\n",
    "    net.eval()\n",
    "    val_loss = 0.0\n",
    "    val_epoch_acc = 0.0\n",
    "    val_cnt = 0\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0.0\n",
    "        logger.info('Validation...')\n",
    "        for inputs, labels in tqdm(val_loader):\n",
    "            # move data from cpu to gpu (if available)\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            acc = calc_accuracy(labels, outputs)\n",
    "            val_epoch_acc += (acc * batch_size)\n",
    "            val_cnt += len(inputs)\n",
    "\n",
    "    # print the epoch loss\n",
    "    train_loss /= len(train_loader)\n",
    "    val_loss /= len(val_loader)\n",
    "\n",
    "    train_acc = epoch_acc/cnt*100\n",
    "    val_acc = val_epoch_acc/val_cnt*100\n",
    "\n",
    "    logger.info(f'Epoch {epoch}: train_loss={train_loss:.4f}, train_acc={train_acc:.2f}%, val_loss={val_loss:.4f}, val_acc={val_acc:.2f}%, lr={scheduler.get_last_lr()[0]}')\n",
    "    writer.add_scalars('loss',{'train':train_loss,'val':val_loss},epoch)\n",
    "    \n",
    "# finally evaluate model on the test set here\n",
    "net.eval()\n",
    "test_loss = 0.0\n",
    "test_epoch_acc = 0.0\n",
    "test_cnt = 0\n",
    "logger.info('Testing...')\n",
    "with torch.no_grad():\n",
    "    test_loss = 0.0\n",
    "    for inputs, labels in tqdm(test_loader):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_loss += loss.item()\n",
    "        acc = calc_accuracy(labels, outputs)\n",
    "        test_epoch_acc += (acc * batch_size)\n",
    "        test_cnt += len(inputs)\n",
    "test_loss /= len(test_loader)\n",
    "test_acc = test_epoch_acc/test_cnt*100\n",
    "logger.info(f'Results: test_loss={test_loss:.4f}, test_acc={test_acc:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model with dynamic lmfcc with different batch size and different learning rate\n",
    "\n",
    "* dynamic lmfcc\n",
    "* active_func = relu\n",
    "* batch_size = 512\n",
    "* hidden_layer_list = [256, 256, 256, 256]\n",
    "* initial_lr = 0.0002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-05-10 10:29:03.184\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m23\u001b[0m - \u001b[1mNet(\n",
      "  (activate): ReLU()\n",
      "  (layer1): Linear(in_features=91, out_features=256, bias=True)\n",
      "  (hidden_layers): Sequential(\n",
      "    (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (5): ReLU()\n",
      "  )\n",
      "  (header_layer): Linear(in_features=256, out_features=61, bias=True)\n",
      ")\u001b[0m\n",
      "\u001b[32m2024-05-10 10:29:03.186\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m24\u001b[0m - \u001b[1mnumber of prameters:\u001b[0m\n",
      "\u001b[32m2024-05-10 10:29:04.610\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 0000 / 2634, loss 0.00695, acc 0.59%, total time [00:00], one epoch time 00:02\u001b[0m\n",
      "\u001b[32m2024-05-10 10:29:05.354\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 0100 / 2634, loss 0.25020, acc 11.33%, total time [00:00], one epoch time 00:19\u001b[0m\n",
      "\u001b[32m2024-05-10 10:29:06.009\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 0200 / 2634, loss 0.07149, acc 27.93%, total time [00:01], one epoch time 00:17\u001b[0m\n",
      "\u001b[32m2024-05-10 10:29:06.664\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 0300 / 2634, loss 0.06235, acc 32.42%, total time [00:02], one epoch time 00:17\u001b[0m\n",
      "\u001b[32m2024-05-10 10:29:07.399\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 0400 / 2634, loss 0.05373, acc 35.74%, total time [00:02], one epoch time 00:19\u001b[0m\n",
      "\u001b[32m2024-05-10 10:29:08.053\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 0500 / 2634, loss 0.04844, acc 41.80%, total time [00:03], one epoch time 00:17\u001b[0m\n",
      "\u001b[32m2024-05-10 10:29:08.709\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 0600 / 2634, loss 0.04566, acc 45.51%, total time [00:04], one epoch time 00:17\u001b[0m\n",
      "\u001b[32m2024-05-10 10:29:09.443\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 0700 / 2634, loss 0.04405, acc 48.05%, total time [00:04], one epoch time 00:19\u001b[0m\n",
      "\u001b[32m2024-05-10 10:29:10.098\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 0800 / 2634, loss 0.04270, acc 51.56%, total time [00:05], one epoch time 00:17\u001b[0m\n",
      "\u001b[32m2024-05-10 10:29:10.753\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 0900 / 2634, loss 0.04133, acc 48.24%, total time [00:06], one epoch time 00:17\u001b[0m\n",
      "\u001b[32m2024-05-10 10:29:11.488\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 1000 / 2634, loss 0.04017, acc 48.63%, total time [00:06], one epoch time 00:19\u001b[0m\n",
      "\u001b[32m2024-05-10 10:29:12.142\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 1100 / 2634, loss 0.03934, acc 51.37%, total time [00:07], one epoch time 00:17\u001b[0m\n",
      "\u001b[32m2024-05-10 10:29:12.797\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 1200 / 2634, loss 0.03844, acc 54.10%, total time [00:08], one epoch time 00:17\u001b[0m\n",
      "\u001b[32m2024-05-10 10:29:13.532\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 1300 / 2634, loss 0.03756, acc 54.69%, total time [00:09], one epoch time 00:19\u001b[0m\n",
      "\u001b[32m2024-05-10 10:29:14.186\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 1400 / 2634, loss 0.03699, acc 52.54%, total time [00:09], one epoch time 00:17\u001b[0m\n",
      "\u001b[32m2024-05-10 10:29:14.841\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 1500 / 2634, loss 0.03636, acc 56.05%, total time [00:10], one epoch time 00:17\u001b[0m\n",
      "\u001b[32m2024-05-10 10:29:15.576\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 1600 / 2634, loss 0.03579, acc 50.78%, total time [00:11], one epoch time 00:19\u001b[0m\n",
      "\u001b[32m2024-05-10 10:29:16.231\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 1700 / 2634, loss 0.03540, acc 59.38%, total time [00:11], one epoch time 00:17\u001b[0m\n",
      "\u001b[32m2024-05-10 10:29:16.883\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 1800 / 2634, loss 0.03510, acc 57.42%, total time [00:12], one epoch time 00:17\u001b[0m\n",
      "\u001b[32m2024-05-10 10:29:17.537\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 1900 / 2634, loss 0.03457, acc 58.01%, total time [00:13], one epoch time 00:17\u001b[0m\n",
      "\u001b[32m2024-05-10 10:29:18.191\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 2000 / 2634, loss 0.03414, acc 60.74%, total time [00:13], one epoch time 00:17\u001b[0m\n",
      "\u001b[32m2024-05-10 10:29:18.925\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 2100 / 2634, loss 0.03408, acc 55.86%, total time [00:14], one epoch time 00:19\u001b[0m\n",
      "\u001b[32m2024-05-10 10:29:19.580\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 2200 / 2634, loss 0.03363, acc 60.74%, total time [00:15], one epoch time 00:17\u001b[0m\n",
      "\u001b[32m2024-05-10 10:29:20.234\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 2300 / 2634, loss 0.03317, acc 62.70%, total time [00:15], one epoch time 00:17\u001b[0m\n",
      "\u001b[32m2024-05-10 10:29:20.888\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 2400 / 2634, loss 0.03301, acc 57.81%, total time [00:16], one epoch time 00:17\u001b[0m\n",
      "\u001b[32m2024-05-10 10:29:21.542\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 2500 / 2634, loss 0.03262, acc 58.59%, total time [00:17], one epoch time 00:17\u001b[0m\n",
      "\u001b[32m2024-05-10 10:29:22.197\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 2600 / 2634, loss 0.03245, acc 59.18%, total time [00:17], one epoch time 00:17\u001b[0m\n",
      "\u001b[32m2024-05-10 10:29:22.587\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m118\u001b[0m - \u001b[1mValidation...\u001b[0m\n",
      "100%|██████████| 311/311 [00:01<00:00, 265.30it/s]\n",
      "\u001b[32m2024-05-10 10:29:23.763\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m137\u001b[0m - \u001b[1mEpoch 0: train_loss=0.0486, train_acc=48.21%, val_loss=0.0330, val_acc=60.27%, lr=0.0002\u001b[0m\n",
      "\u001b[32m2024-05-10 10:29:23.764\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m145\u001b[0m - \u001b[1mTesting...\u001b[0m\n",
      "100%|██████████| 2983/2983 [00:11<00:00, 254.35it/s]\n",
      "\u001b[32m2024-05-10 10:29:35.496\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mResults: test_loss=0.0336, test_acc=59.62%\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# set parameters and initiate net\n",
    "use_dynamic_features = True\n",
    "feature_type = 'lmfcc'\n",
    "activate_func = 'relu'\n",
    "batch_size = 512\n",
    "hidden_layer_list = [256, 256, 256, 256]\n",
    "initial_lr = 0.0002\n",
    "num_epochs = 1\n",
    "if feature_type == 'lmfcc':\n",
    "    feat_dim = 13\n",
    "elif feature_type == 'mspec':\n",
    "    feat_dim = 40\n",
    "\n",
    "if use_dynamic_features:\n",
    "    input_size = feat_dim * 7\n",
    "else:\n",
    "    input_size = feat_dim\n",
    "\n",
    "stateList = np.load('state_list.npy').tolist()\n",
    "num_cls = len(stateList)\n",
    "\n",
    "net = Net(input_size=input_size, num_cls=num_cls, hidden_layer_list=hidden_layer_list, activate_func=activate_func)\n",
    "logger.info(net)\n",
    "logger.info('number of prameters:', count_parameters(net))\n",
    "net = net.to(device)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=initial_lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=40, gamma=0.1)\n",
    "\n",
    "# load preprocessed data\n",
    "prepared_data_train = np.load('prepared_train_data.npz')\n",
    "if use_dynamic_features:\n",
    "    np_train_x = prepared_data_train[f'data_x_dynamic_{feature_type}']\n",
    "    np_train_y = prepared_data_train['data_y']\n",
    "else:\n",
    "    np_train_x = prepared_data_train[f'data_x_{feature_type}']\n",
    "    np_train_y = prepared_data_train['data_y']\n",
    "train_x = torch.tensor(np_train_x)\n",
    "train_y = F.one_hot(torch.tensor(np_train_y, dtype=torch.long), num_classes=num_cls).float()\n",
    "\n",
    "prepared_data_val = np.load('prepared_val_data.npz')\n",
    "if use_dynamic_features:\n",
    "    np_val_x = prepared_data_val[f'data_x_dynamic_{feature_type}']\n",
    "    np_val_y = prepared_data_val['data_y']\n",
    "else:\n",
    "    np_val_x = prepared_data_val[f'data_x_{feature_type}']\n",
    "    np_val_y = prepared_data_val['data_y']\n",
    "val_x = torch.tensor(np_val_x)\n",
    "val_y = F.one_hot(torch.tensor(np_val_y, dtype=torch.long), num_classes=num_cls).float()\n",
    "    \n",
    "prepared_data_test = np.load('prepared_test_data.npz')\n",
    "if use_dynamic_features:\n",
    "    np_test_x = prepared_data_test[f'data_x_dynamic_{feature_type}']\n",
    "    np_test_y = prepared_data_test['data_y']\n",
    "else:\n",
    "    np_test_x = prepared_data_test[f'data_x_{feature_type}']\n",
    "    np_test_y = prepared_data_test['data_y']\n",
    "test_x = torch.tensor(np_test_x)\n",
    "test_y = F.one_hot(torch.tensor(np_test_y, dtype=torch.long), num_classes=num_cls).float()\n",
    "\n",
    "# create the data loaders for training and validation sets\n",
    "train_dataset = torch.utils.data.TensorDataset(train_x, train_y)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataset = torch.utils.data.TensorDataset(val_x, val_y)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_dataset = torch.utils.data.TensorDataset(test_x, test_y)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# train the network\n",
    "st = time.time()\n",
    "last_st = time.time()\n",
    "log_interval = 100\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    net.train()\n",
    "    train_loss = 0.0\n",
    "    sub_train_loss = 0.0\n",
    "    epoch_acc = 0.0\n",
    "    cnt = 0\n",
    "    for idx, (inputs, labels) in enumerate(train_loader):\n",
    "\n",
    "        # move data from cpu to gpu (if available)\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # accumulate the training loss\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        sub_train_loss += loss.item()\n",
    "\n",
    "        if idx % log_interval == 0:\n",
    "            duration = time.time() - st\n",
    "            acc = calc_accuracy(labels, outputs)\n",
    "            epoch_acc += (acc * batch_size)\n",
    "            cnt += batch_size\n",
    "            logger.info(f'epoch {epoch:03d} / {num_epochs}, step {idx:04d} / {len(train_loader)}, loss {sub_train_loss / log_interval:.5f}, acc {acc*100:.2f}%, total time [{format_time(duration)}], one epoch time {format_time(len(train_loader) / log_interval * (time.time() - last_st))}')\n",
    "            sub_train_loss = 0.0\n",
    "            last_st = time.time()\n",
    "\n",
    "    # update the learning rate\n",
    "    scheduler.step()\n",
    "\n",
    "    # calculate the validation loss\n",
    "    net.eval()\n",
    "    val_loss = 0.0\n",
    "    val_epoch_acc = 0.0\n",
    "    val_cnt = 0\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0.0\n",
    "        logger.info('Validation...')\n",
    "        for inputs, labels in tqdm(val_loader):\n",
    "            # move data from cpu to gpu (if available)\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            acc = calc_accuracy(labels, outputs)\n",
    "            val_epoch_acc += (acc * batch_size)\n",
    "            val_cnt += len(inputs)\n",
    "\n",
    "    # print the epoch loss\n",
    "    train_loss /= len(train_loader)\n",
    "    val_loss /= len(val_loader)\n",
    "\n",
    "    train_acc = epoch_acc/cnt*100\n",
    "    val_acc = val_epoch_acc/val_cnt*100\n",
    "\n",
    "    logger.info(f'Epoch {epoch}: train_loss={train_loss:.4f}, train_acc={train_acc:.2f}%, val_loss={val_loss:.4f}, val_acc={val_acc:.2f}%, lr={scheduler.get_last_lr()[0]}')\n",
    "    writer.add_scalars('loss',{'train':train_loss,'val':val_loss},epoch)\n",
    "    \n",
    "# finally evaluate model on the test set here\n",
    "net.eval()\n",
    "test_loss = 0.0\n",
    "test_epoch_acc = 0.0\n",
    "test_cnt = 0\n",
    "logger.info('Testing...')\n",
    "with torch.no_grad():\n",
    "    test_loss = 0.0\n",
    "    for inputs, labels in tqdm(test_loader):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_loss += loss.item()\n",
    "        acc = calc_accuracy(labels, outputs)\n",
    "        test_epoch_acc += (acc * batch_size)\n",
    "        test_cnt += len(inputs)\n",
    "test_loss /= len(test_loader)\n",
    "test_acc = test_epoch_acc/test_cnt*100\n",
    "logger.info(f'Results: test_loss={test_loss:.4f}, test_acc={test_acc:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model with dynamic lmfcc with different learning rate\n",
    "\n",
    "* dynamic lmfcc\n",
    "* active_func = relu\n",
    "* batch_size = 256\n",
    "* hidden_layer_list = [256, 256, 256, 256]\n",
    "* initial_lr = 0.0002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-05-10 10:30:06.368\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m23\u001b[0m - \u001b[1mNet(\n",
      "  (activate): ReLU()\n",
      "  (layer1): Linear(in_features=91, out_features=256, bias=True)\n",
      "  (hidden_layers): Sequential(\n",
      "    (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (5): ReLU()\n",
      "  )\n",
      "  (header_layer): Linear(in_features=256, out_features=61, bias=True)\n",
      ")\u001b[0m\n",
      "\u001b[32m2024-05-10 10:30:06.370\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m24\u001b[0m - \u001b[1mnumber of prameters:\u001b[0m\n",
      "\u001b[32m2024-05-10 10:30:07.834\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 0000 / 5267, loss 0.00690, acc 0.00%, total time [00:00], one epoch time 00:04\u001b[0m\n",
      "\u001b[32m2024-05-10 10:30:08.414\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 0100 / 5267, loss 0.25903, acc 15.62%, total time [00:00], one epoch time 00:30\u001b[0m\n",
      "\u001b[32m2024-05-10 10:30:08.984\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 0200 / 5267, loss 0.07235, acc 26.95%, total time [00:01], one epoch time 00:29\u001b[0m\n",
      "\u001b[32m2024-05-10 10:30:09.613\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 0300 / 5267, loss 0.06233, acc 33.59%, total time [00:01], one epoch time 00:33\u001b[0m\n",
      "\u001b[32m2024-05-10 10:30:10.137\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 0400 / 5267, loss 0.05344, acc 37.89%, total time [00:02], one epoch time 00:27\u001b[0m\n",
      "\u001b[32m2024-05-10 10:30:10.675\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 0500 / 5267, loss 0.04866, acc 41.41%, total time [00:02], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:30:11.226\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 0600 / 5267, loss 0.04580, acc 52.34%, total time [00:03], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:30:11.777\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 0700 / 5267, loss 0.04425, acc 44.53%, total time [00:04], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:30:12.330\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 0800 / 5267, loss 0.04291, acc 44.53%, total time [00:04], one epoch time 00:29\u001b[0m\n",
      "\u001b[32m2024-05-10 10:30:12.888\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 0900 / 5267, loss 0.04147, acc 52.34%, total time [00:05], one epoch time 00:29\u001b[0m\n",
      "\u001b[32m2024-05-10 10:30:13.440\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 1000 / 5267, loss 0.04065, acc 46.88%, total time [00:05], one epoch time 00:29\u001b[0m\n",
      "\u001b[32m2024-05-10 10:30:14.005\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 1100 / 5267, loss 0.03981, acc 52.73%, total time [00:06], one epoch time 00:29\u001b[0m\n",
      "\u001b[32m2024-05-10 10:30:14.573\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 1200 / 5267, loss 0.03939, acc 55.08%, total time [00:06], one epoch time 00:29\u001b[0m\n",
      "\u001b[32m2024-05-10 10:30:15.140\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 1300 / 5267, loss 0.03881, acc 51.56%, total time [00:07], one epoch time 00:29\u001b[0m\n",
      "\u001b[32m2024-05-10 10:30:15.769\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 1400 / 5267, loss 0.03826, acc 58.98%, total time [00:08], one epoch time 00:33\u001b[0m\n",
      "\u001b[32m2024-05-10 10:30:16.307\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 1500 / 5267, loss 0.03735, acc 55.08%, total time [00:08], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:30:16.861\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 1600 / 5267, loss 0.03725, acc 54.69%, total time [00:09], one epoch time 00:29\u001b[0m\n",
      "\u001b[32m2024-05-10 10:30:17.425\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 1700 / 5267, loss 0.03683, acc 58.98%, total time [00:09], one epoch time 00:29\u001b[0m\n",
      "\u001b[32m2024-05-10 10:30:17.992\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 1800 / 5267, loss 0.03612, acc 57.03%, total time [00:10], one epoch time 00:29\u001b[0m\n",
      "\u001b[32m2024-05-10 10:30:18.556\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 1900 / 5267, loss 0.03600, acc 58.20%, total time [00:10], one epoch time 00:29\u001b[0m\n",
      "\u001b[32m2024-05-10 10:30:19.120\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 2000 / 5267, loss 0.03561, acc 55.86%, total time [00:11], one epoch time 00:29\u001b[0m\n",
      "\u001b[32m2024-05-10 10:30:19.684\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 2100 / 5267, loss 0.03482, acc 59.38%, total time [00:11], one epoch time 00:29\u001b[0m\n",
      "\u001b[32m2024-05-10 10:30:20.249\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 2200 / 5267, loss 0.03490, acc 58.98%, total time [00:12], one epoch time 00:29\u001b[0m\n",
      "\u001b[32m2024-05-10 10:30:20.810\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 2300 / 5267, loss 0.03450, acc 55.86%, total time [00:13], one epoch time 00:29\u001b[0m\n",
      "\u001b[32m2024-05-10 10:30:21.359\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 2400 / 5267, loss 0.03412, acc 57.03%, total time [00:13], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:30:21.969\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 2500 / 5267, loss 0.03399, acc 59.38%, total time [00:14], one epoch time 00:32\u001b[0m\n",
      "\u001b[32m2024-05-10 10:30:22.504\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 2600 / 5267, loss 0.03372, acc 61.33%, total time [00:14], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:30:23.038\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 2700 / 5267, loss 0.03348, acc 60.94%, total time [00:15], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:30:23.574\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 2800 / 5267, loss 0.03334, acc 62.89%, total time [00:15], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:30:24.149\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 2900 / 5267, loss 0.03295, acc 53.12%, total time [00:16], one epoch time 00:30\u001b[0m\n",
      "\u001b[32m2024-05-10 10:30:24.763\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 3000 / 5267, loss 0.03299, acc 62.50%, total time [00:17], one epoch time 00:32\u001b[0m\n",
      "\u001b[32m2024-05-10 10:30:25.285\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 3100 / 5267, loss 0.03250, acc 59.77%, total time [00:17], one epoch time 00:27\u001b[0m\n",
      "\u001b[32m2024-05-10 10:30:25.817\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 3200 / 5267, loss 0.03285, acc 64.06%, total time [00:18], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:30:26.362\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 3300 / 5267, loss 0.03222, acc 59.77%, total time [00:18], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:30:26.908\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 3400 / 5267, loss 0.03231, acc 62.50%, total time [00:19], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:30:27.453\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 3500 / 5267, loss 0.03182, acc 62.11%, total time [00:19], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:30:28.060\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 3600 / 5267, loss 0.03146, acc 62.50%, total time [00:20], one epoch time 00:31\u001b[0m\n",
      "\u001b[32m2024-05-10 10:30:28.600\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 3700 / 5267, loss 0.03164, acc 58.98%, total time [00:20], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:30:29.140\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 3800 / 5267, loss 0.03160, acc 63.67%, total time [00:21], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:30:29.678\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 3900 / 5267, loss 0.03131, acc 66.80%, total time [00:21], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:30:30.218\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 4000 / 5267, loss 0.03159, acc 63.28%, total time [00:22], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:30:30.758\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 4100 / 5267, loss 0.03117, acc 60.55%, total time [00:23], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:30:31.308\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 4200 / 5267, loss 0.03072, acc 62.89%, total time [00:23], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:30:31.857\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 4300 / 5267, loss 0.03086, acc 65.23%, total time [00:24], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:30:32.407\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 4400 / 5267, loss 0.03122, acc 61.33%, total time [00:24], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:30:32.957\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 4500 / 5267, loss 0.03043, acc 67.97%, total time [00:25], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:30:33.582\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 4600 / 5267, loss 0.03041, acc 58.59%, total time [00:25], one epoch time 00:32\u001b[0m\n",
      "\u001b[32m2024-05-10 10:30:34.105\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 4700 / 5267, loss 0.03030, acc 65.23%, total time [00:26], one epoch time 00:27\u001b[0m\n",
      "\u001b[32m2024-05-10 10:30:34.640\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 4800 / 5267, loss 0.03049, acc 66.80%, total time [00:26], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:30:35.177\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 4900 / 5267, loss 0.02993, acc 66.02%, total time [00:27], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:30:35.715\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 5000 / 5267, loss 0.03015, acc 64.84%, total time [00:27], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:30:36.252\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 5100 / 5267, loss 0.02964, acc 71.09%, total time [00:28], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:30:36.789\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mepoch 000 / 1, step 5200 / 5267, loss 0.02979, acc 58.98%, total time [00:29], one epoch time 00:28\u001b[0m\n",
      "\u001b[32m2024-05-10 10:30:37.238\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m118\u001b[0m - \u001b[1mValidation...\u001b[0m\n",
      "100%|██████████| 622/622 [00:01<00:00, 398.25it/s]\n",
      "\u001b[32m2024-05-10 10:30:38.804\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m137\u001b[0m - \u001b[1mEpoch 0: train_loss=0.0406, train_acc=55.45%, val_loss=0.0305, val_acc=63.49%, lr=0.0002\u001b[0m\n",
      "\u001b[32m2024-05-10 10:30:38.805\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m145\u001b[0m - \u001b[1mTesting...\u001b[0m\n",
      "100%|██████████| 5965/5965 [00:14<00:00, 406.40it/s]\n",
      "\u001b[32m2024-05-10 10:30:53.488\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mResults: test_loss=0.0312, test_acc=62.59%\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# set parameters and initiate net\n",
    "use_dynamic_features = True\n",
    "feature_type = 'lmfcc'\n",
    "activate_func = 'relu'\n",
    "batch_size = 256\n",
    "hidden_layer_list = [256, 256, 256, 256]\n",
    "initial_lr = 0.0002\n",
    "num_epochs = 1\n",
    "if feature_type == 'lmfcc':\n",
    "    feat_dim = 13\n",
    "elif feature_type == 'mspec':\n",
    "    feat_dim = 40\n",
    "\n",
    "if use_dynamic_features:\n",
    "    input_size = feat_dim * 7\n",
    "else:\n",
    "    input_size = feat_dim\n",
    "\n",
    "stateList = np.load('state_list.npy').tolist()\n",
    "num_cls = len(stateList)\n",
    "\n",
    "net = Net(input_size=input_size, num_cls=num_cls, hidden_layer_list=hidden_layer_list, activate_func=activate_func)\n",
    "logger.info(net)\n",
    "logger.info('number of prameters:', count_parameters(net))\n",
    "net = net.to(device)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=initial_lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=40, gamma=0.1)\n",
    "\n",
    "# load preprocessed data\n",
    "prepared_data_train = np.load('prepared_train_data.npz')\n",
    "if use_dynamic_features:\n",
    "    np_train_x = prepared_data_train[f'data_x_dynamic_{feature_type}']\n",
    "    np_train_y = prepared_data_train['data_y']\n",
    "else:\n",
    "    np_train_x = prepared_data_train[f'data_x_{feature_type}']\n",
    "    np_train_y = prepared_data_train['data_y']\n",
    "train_x = torch.tensor(np_train_x)\n",
    "train_y = F.one_hot(torch.tensor(np_train_y, dtype=torch.long), num_classes=num_cls).float()\n",
    "\n",
    "prepared_data_val = np.load('prepared_val_data.npz')\n",
    "if use_dynamic_features:\n",
    "    np_val_x = prepared_data_val[f'data_x_dynamic_{feature_type}']\n",
    "    np_val_y = prepared_data_val['data_y']\n",
    "else:\n",
    "    np_val_x = prepared_data_val[f'data_x_{feature_type}']\n",
    "    np_val_y = prepared_data_val['data_y']\n",
    "val_x = torch.tensor(np_val_x)\n",
    "val_y = F.one_hot(torch.tensor(np_val_y, dtype=torch.long), num_classes=num_cls).float()\n",
    "    \n",
    "prepared_data_test = np.load('prepared_test_data.npz')\n",
    "if use_dynamic_features:\n",
    "    np_test_x = prepared_data_test[f'data_x_dynamic_{feature_type}']\n",
    "    np_test_y = prepared_data_test['data_y']\n",
    "else:\n",
    "    np_test_x = prepared_data_test[f'data_x_{feature_type}']\n",
    "    np_test_y = prepared_data_test['data_y']\n",
    "test_x = torch.tensor(np_test_x)\n",
    "test_y = F.one_hot(torch.tensor(np_test_y, dtype=torch.long), num_classes=num_cls).float()\n",
    "\n",
    "# create the data loaders for training and validation sets\n",
    "train_dataset = torch.utils.data.TensorDataset(train_x, train_y)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataset = torch.utils.data.TensorDataset(val_x, val_y)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_dataset = torch.utils.data.TensorDataset(test_x, test_y)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# train the network\n",
    "st = time.time()\n",
    "last_st = time.time()\n",
    "log_interval = 100\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    net.train()\n",
    "    train_loss = 0.0\n",
    "    sub_train_loss = 0.0\n",
    "    epoch_acc = 0.0\n",
    "    cnt = 0\n",
    "    for idx, (inputs, labels) in enumerate(train_loader):\n",
    "\n",
    "        # move data from cpu to gpu (if available)\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # accumulate the training loss\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        sub_train_loss += loss.item()\n",
    "\n",
    "        if idx % log_interval == 0:\n",
    "            duration = time.time() - st\n",
    "            acc = calc_accuracy(labels, outputs)\n",
    "            epoch_acc += (acc * batch_size)\n",
    "            cnt += batch_size\n",
    "            logger.info(f'epoch {epoch:03d} / {num_epochs}, step {idx:04d} / {len(train_loader)}, loss {sub_train_loss / log_interval:.5f}, acc {acc*100:.2f}%, total time [{format_time(duration)}], one epoch time {format_time(len(train_loader) / log_interval * (time.time() - last_st))}')\n",
    "            sub_train_loss = 0.0\n",
    "            last_st = time.time()\n",
    "\n",
    "    # update the learning rate\n",
    "    scheduler.step()\n",
    "\n",
    "    # calculate the validation loss\n",
    "    net.eval()\n",
    "    val_loss = 0.0\n",
    "    val_epoch_acc = 0.0\n",
    "    val_cnt = 0\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0.0\n",
    "        logger.info('Validation...')\n",
    "        for inputs, labels in tqdm(val_loader):\n",
    "            # move data from cpu to gpu (if available)\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            acc = calc_accuracy(labels, outputs)\n",
    "            val_epoch_acc += (acc * batch_size)\n",
    "            val_cnt += len(inputs)\n",
    "\n",
    "    # print the epoch loss\n",
    "    train_loss /= len(train_loader)\n",
    "    val_loss /= len(val_loader)\n",
    "\n",
    "    train_acc = epoch_acc/cnt*100\n",
    "    val_acc = val_epoch_acc/val_cnt*100\n",
    "\n",
    "    logger.info(f'Epoch {epoch}: train_loss={train_loss:.4f}, train_acc={train_acc:.2f}%, val_loss={val_loss:.4f}, val_acc={val_acc:.2f}%, lr={scheduler.get_last_lr()[0]}')\n",
    "    writer.add_scalars('loss',{'train':train_loss,'val':val_loss},epoch)\n",
    "    \n",
    "# finally evaluate model on the test set here\n",
    "net.eval()\n",
    "test_loss = 0.0\n",
    "test_epoch_acc = 0.0\n",
    "test_cnt = 0\n",
    "logger.info('Testing...')\n",
    "with torch.no_grad():\n",
    "    test_loss = 0.0\n",
    "    for inputs, labels in tqdm(test_loader):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_loss += loss.item()\n",
    "        acc = calc_accuracy(labels, outputs)\n",
    "        test_epoch_acc += (acc * batch_size)\n",
    "        test_cnt += len(inputs)\n",
    "test_loss /= len(test_loader)\n",
    "test_acc = test_epoch_acc/test_cnt*100\n",
    "logger.info(f'Results: test_loss={test_loss:.4f}, test_acc={test_acc:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "speech",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
